= Amazon AWS S3

Amazon AWS S3 Object Storage is a production-worthy choice for both a
data source and a data sink in a batch computation job.

The S3 Source and Sink uses Amazon S3 SDK to connect to the storage.
The connectors expect the user to provide either an S3Client
client instance or credentials (Access key ID, Secret Access Key) to
create the client. The source and sink assume the data is in the form
of plain text and emit/receive data items which represent individual
lines of text.

The connectors are not fault-tolerant. On job restart they behave as if
you started a new job. The source does not do snapshotting. The sink
always overwrites previously written objects.

== S3 Source

S3 source distributes the workload by splitting the objects to each
processor instance. The source reads each object line by line and emits
desired output to downstream using provided transform function.

Below example will log on each Jet member the contents of all the
objects in the specified bucket. When the source reads all the files,
the job completes. If the objects change while the job is running, the
behavior is undefined.
[source]
----
include::{javasource}/integration/S3.java[tag=s1]
----

== S3 Sink

S3 sink writes items to the specified bucket by transforming each item
to a line using given transform function. The sink creates an object
in the bucket for each processor instance. Name of the file will
include a user provided prefix (if defined) and followed by the
processor's global index, for example the processor having the index
 `2` with prefix `my-object-` will create the object `my-object-2`.

Here's a small example of usage:

[source]
----
include::{javasource}/integration/S3.java[tag=s2]
----

S3 sink uses multi-part upload feature of S3 SDK. The sink buffers the
items to parts and uploads them after buffer reaches to the threshold.
The multi-part upload is completed when the job completes and makes the
objects available on the S3. Since streaming jobs never complete, S3
sink is not applicable for streaming jobs. 

== Using HDFS as S3 connector

AWS S3 can be registered as a filesystem to HDFS which then can be used
as a source or sink via our hadoop module. See
https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html[Hadoop-AWS module]
for more information on the dependencies and configuration.

Below example will read from a bucket, transform each line to uppercase
and write to another bucket using the hadoop module.

[source]
----
include::{javasource}/integration/S3.java[tag=s3]
----