= Kafka

Apache Kafka is a production-worthy choice for both source and sink
for infinite stream processing jobs. It supports fault tolerance and
snapshotting. The basic paradigm is that of a distributed
publish/subscribe topic. Jet's Kafka Source subscribes to a Kafka topic
and the sink publishes events to a Kafka topic.

The following code will consume from topics `t1` and `t2` and then write
to `t3`:

[source]
----
include::{javasource}/integration/HdfsAndKafka.java[tag=s5]
----

== Using Kafka as a Source

The Kafka source emits entries of type `Map.Entry<Key,Value>` which can
be transformed using an optional mapping function. It never completes.
The job will end only if explicitly cancelled or aborted due to an
error.

Internally Jet creates one `KafkaConsumer` per `Processor` instance
using the supplied properties. Jet uses manual partition assignment to
arrange the available Kafka partitions among the available processors
and will ignore the `group.id` property.

If any new partitions are added while the job is running, Jet will
automatically assign them to the existing processors and consume them
from the beginning.

== Processing Guarantees

The Kafka source supports snapshots. Upon each snapshot it saves the
current offset for each partition. When the job is restarted from a
snapshot, the source can continue reading from the saved offsets.

If processing guarantee is disabled, the source will start reading from
default offsets (based on the `auto.offset.reset` property). You can
enable offset committing by assigning a `group.id`, enabling auto offset
committing using `enable.auto.commit` and configuring
`auto.commit.interval.ms` in the given properties. Refer to Kafka
documentation for the descriptions of these properties.

== Using Kafka as a Sink

The sink provides the exactly-once guarantee at the cost of using Kafka
transactions: Jet commits the produced records after each snapshot is
completed. This greatly increases the latency because consumers see the
records only after they are committed.

If you use _at-least-once_ guarantee, records are visible immediately,
but in the case of a failure some records could be produced duplicately.
You can also have the job in exactly-once mode and decrease the
guarantee just for a particular Kafka sink.

== Limitations

Apache Kafka introduced client backward compatibility with version 1.0.0.
The compatibility is `two way`, new brokers support older clients and
new clients support older broker.

The Kafka sink and source are based on version 2.2.0, this means Kafka
connector will work with any client and broker having version equal to
or greater than 1.0.0.


