= Hadoop

The Hadoop connector is a production-worthy choice for
both a data source and a data sink in a batch computation job. 

You can use Hadoop connector to read/write files from/to Hadoop 
Distributed File System (HDFS), local file system, or any Hadoop Compatible 
File System including various cloud storages. Jet was tested with:

 - Amazon S3
 - Google Cloud Storage
 - Azure Cloud Storage
 - Azure Data Lake
 
For more information you can check the
{jet-examples}/hadoop/src/main/java/com/hazelcast/jet/examples/hadoop[code samples]
which includes `Avro` and `Parquet` file format examples and different cloud storage
options.

== Configuration

The Hadoop source and sink require a configuration object of type
https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/conf/Configuration.html[Configuration]
which supplies the input and output paths and formats. They don't
actually create a MapReduce job, this config is simply used to describe
the required inputs and outputs. You can share the same `Configuration`
instance between several source/sink instances.

The Hadoop source and sink will use either the new or the old MapReduce
API based on the input format configuration. If it is stored under
`MRJobConfig.INPUT_FORMAT_CLASS_ATTR` (`mapreduce.job.inputformat.class`)
the new API will be used, otherwise the old API will be used.

Here's a configuration sample for old API:

[source]
----
include::{javasource}/integration/HdfsAndKafka.java[tag=s1]
----

Here's a configuration sample for new API:

[source]
----
include::{javasource}/integration/HdfsAndKafka.java[tag=s2]
----

With the new API, most of the `RecordReader` implementations return the
same key/value instances for each record, aiming to lessen object
creation. Hazelcast Jet expects the objects to be immutable thus makes a
copy of each object after the configured projection function applied to
the record. The source copies the objects by serializing and
de-serializing them. The objects should be either
https://hadoop.apache.org/docs/r2.8.3/api/org/apache/hadoop/io/Writable.html[Writable]
or serializable in a way which Hazelcast Jet can serialize/de-serialize.

For readers which create a new instance for each record, the source can
be configured to not copy the objects for performance. Also if the
configured projection function doesn't refer to any mutable state from
the key or value, again the source can be configured to not copy the
objects.

Here's a configuration sample which disables copying objects:

[source]
----
include::{javasource}/integration/HdfsAndKafka.java[tag=s3]
----

The word count pipeline can then be expressed using Hadoop connector as follows

[source]
----
include::{javasource}/integration/HdfsAndKafka.java[tag=s4]
----

== Data Locality When Reading

Jet will split the input data across the cluster, with each processor
instance reading a part of the input. If the Jet nodes are running along
the Hadoop datanodes, then Jet can make use of data locality by reading
the blocks locally where possible. This can bring a significant increase
in read speed.

== Output

Each processor will write to a different file in the output folder
identified by the unique processor id. The files will be in a temporary
state until the job is completed and will be committed when the job is
complete. For streaming jobs, they will be committed when the job is
cancelled. We have plans to introduce a rolling sink for Hadoop in the
future to have better streaming support.

== Dealing with Writables

Hadoop types implement their own serialization mechanism through the use
of https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html[Writable].
Jet provides an adapter to register a `Writable` for
{hz-refman}#serialization[Hazelcast serialization]
without having to write additional serialization code. To use this
adapter, you can register your own `Writable` types by extending
`WritableSerializerHook` and
{hz-refman}#serialization-configuration-wrap-up[registering the hook].

== Hadoop JARs and Classpath

When submitting JARs along with a Job, sending Hadoop JARs should be
avoided and instead Hadoop JARs should be present on the classpath of
the running members. Hadoop JARs contain some JVM hooks and can keep
lingering references inside the JVM long after the job has ended,
causing memory leaks.
