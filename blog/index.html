<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Blog · Hazelcast Jet</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="Open-Source Distributed Stream Processing"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Blog · Hazelcast Jet"/><meta property="og:type" content="website"/><meta property="og:url" content="https://jet-start.sh/"/><meta property="og:description" content="Open-Source Distributed Stream Processing"/><meta property="og:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://jet-start.sh/blog/atom.xml" title="Hazelcast Jet Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://jet-start.sh/blog/feed.xml" title="Hazelcast Jet Blog RSS Feed"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-158279495-1', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,500,600"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600,700,800"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-block-buttons.js"></script><script type="text/javascript" src="https://plausible.io/js/plausible.js" async="" defer="" data-domain="jet-start.sh"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/prism.css"/><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="blog"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo-dark.svg" alt="Hazelcast Jet"/></a><a href="/versions"><h3>4.2</h3></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/get-started/intro" target="_self">Docs</a></li><li class=""><a href="/download" target="_self">Download</a></li><li class=""><a href="/demos" target="_self">Demos</a></li><li class=""><a href="https://github.com/hazelcast/hazelcast-jet" target="_self">GitHub</a></li><li class=""><a href="https://slack.hazelcast.com/" target="_self">Community</a></li><li class="siteNavGroupActive siteNavItemActive"><a href="/blog/" target="_self">Blog</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>All posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">All posts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/08/05/gc-tuning-for-jet">Sub-10 ms Latency in Java: Concurrent GC with Green Threads</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/16/designing-evergreen-cache-cdc">Designing an Evergreen Cache with Change Data Capture</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/14/jet-42-is-released">Jet 4.2 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Performance of Modern Java on Data-Heavy Workloads: Batch Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/25/grcp">Processing 10M queries / second on a single node using Jet and gRPC</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/18/spark-jet">How Hazelcast Jet Compares to Apache Spark</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/29/jet-41-is-released">Jet 4.1 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/01/upgrading-to-jet-40">Upgrading to Jet 4.0</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/30/ml-inference">Machine Learning Inference at Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/02/jet-40-is-released">Jet 4.0 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/02/20/transactional-processors">Transactional connectors in Hazelcast Jet</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/01/28/new-website">Announcing New Documentation Website</a></li><li class="navListItem"><a class="navItem" href="/blog/2019/11/12/stream-deduplication">Stream Deduplication with Hazelcast Jet</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="posts"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a></h1><p class="post-meta">September 18, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://www.linkedin.com/in/bjozsef/" target="_blank" rel="noreferrer noopener">Bartók József</a></p><div class="authorPhoto"><a href="https://www.linkedin.com/in/bjozsef/" target="_blank" rel="noreferrer noopener"><img src="https://www.itdays.ro/public/images/speakers-big/Jozsef_Bartok.jpg" alt="Bartók József"/></a></div></div></header><article class="post-content"><div><span><h2><a class="anchor" aria-hidden="true" id="introduction"></a><a href="#introduction" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p><em>Change Data Capture</em> (CDC) refers to the process of <em>observing
changes made to a database</em> and extracting them in a form usable by
other systems, for the purposes of replication, analysis and many more.</p>
<p><em>Hazelcast Jet</em> is a distributed, lightweight stream processing
framework. It allows you to write modern Java code that focuses purely
on data transformation while it does all the heavy lifting of getting
the data flowing and computation running across a cluster of nodes. Jet
stores computational state in <a href="https://jet-start.sh/docs/api/data-structures">fault-tolerant, distributed in-memory
storage</a>, allowing
thousands of concurrent users granular and fast access to your data
without breaking a sweat.</p>
<p>While stream processing is a natural solution for providing insight into
many big-data workloads, it’s a relatively new evolution over its
predecessor - offline batch processing. Utilizing stream processing
effectively requires re-architecting existing systems to event-driven
architectures and introducing several new components. This process is
not always straightforward and also requires a shift in mindset.</p>
<p>In this context, the functionality provided by change data capture
technologies, for which Debezium is one of the, if not THE best
open-source alternative, is a godsend. To be able to ingest data from
relational databases, without affecting the applications that use them,
changes the game for streaming systems. It becomes possible to safely
extend old systems with all kinds of new functionality: real-time
analytics, complex event processing, anomaly &amp; fraud detection and so
on.</p>
<h2><a class="anchor" aria-hidden="true" id="integration"></a><a href="#integration" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Integration</h2>
<p>When we first considered integrating Debezium into Jet, the most
important decisions were centered around the fact that Debezium is
designed to be deployed via Apache <a href="https://kafka.apache.org/documentation/#connect">Kafka
Connect</a>, which then
takes care of <em>fault tolerance</em> and <em>scalability</em>. Fortunately, Jet is
fully capable of providing these crucial services. Also, Kafka Connect
is a good enough abstraction that we were able to mimic it for Debezium.</p>
<p>We are aware that Debezium also offers an <em>embedded mode</em> for
applications not interested in fault-tolerance guarantees such as
exactly-once processing and resilience, but since Jet does not have a
“dumbed down” version (even as full-blown is light enough to be
embedded), we quickly discarded this approach.</p>
<p>So, first, we added generic support for Kafka Connect sources to Jet,
which should be a valuable feature even outside the scope of CDC. Then
we used Debezium to build a Kafka Connect source for Jet. Well… “build”
might be overstating it. Debezium already is a Kafka Connect source. We
just had to make sure that Jet’s specific fault-tolerance mechanisms
will interact with it properly, through the Kafka Connect API.</p>
<h2><a class="anchor" aria-hidden="true" id="synergy"></a><a href="#synergy" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Synergy</h2>
<p>One immediate benefit that Jet offers to Debezium users is eliminating
the need for <em>external services</em>. No Zookeeper, no Kafka needed. When
using Debezium through Jet, the latter takes care of the whole lifecycle
and fault tolerance of all the components involved. The setup is greatly
simplified.</p>
<p>Then, obviously, there is the <em>stream processing capability</em>, because
that’s what Jet does. Not only do you get access to the data, but you
also have the toolbox to process it, extract whatever insights you need
from it.</p>
<p>In addition, Jet also aims to offer <em>further convenience</em> wrappers when
the Debezium source is being used. For example:</p>
<ul>
<li>builders for the most common configuration properties to make setting
up Debezium for some specific DB as simple as possible</li>
<li>standard Java interfaces to give structure to the complex Debezium
events</li>
<li>JSON parsing, including mapping to Objects, based on <a href="https://github.com/FasterXML/jackson-jr">Jackson
jr</a>, to simplify how parts
of - or even entire Debezium events can be interpreted</li>
</ul>
<p>For an example look at this sample from our <a href="https://jet-start.sh/docs/tutorials/cdc#6-define-jet-job">CDC
tutorial</a>. All
the code you would need to build an in-memory replica of your MySQL
database table would be something like:</p>
<pre><code class="hljs css language-java"><span class="token class-name">StreamSource</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">ChangeRecord</span><span class="token punctuation">></span></span> source <span class="token operator">=</span> <span class="token class-name">MySqlCdcSources</span><span class="token punctuation">.</span><span class="token function">mysql</span><span class="token punctuation">(</span><span class="token string">"source"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setDatabaseAddress</span><span class="token punctuation">(</span><span class="token string">"127.0.0.1"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setDatabasePort</span><span class="token punctuation">(</span><span class="token number">3306</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setDatabaseUser</span><span class="token punctuation">(</span><span class="token string">"debezium"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setDatabasePassword</span><span class="token punctuation">(</span><span class="token string">"dbz"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setClusterName</span><span class="token punctuation">(</span><span class="token string">"dbserver1"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setDatabaseWhitelist</span><span class="token punctuation">(</span><span class="token string">"inventory"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setTableWhitelist</span><span class="token punctuation">(</span><span class="token string">"inventory.customers"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token class-name">Pipeline</span> pipeline <span class="token operator">=</span> <span class="token class-name">Pipeline</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
pipeline<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span>source<span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">withoutTimestamps</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">CdcSinks</span><span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token string">"customers"</span><span class="token punctuation">,</span>
                r <span class="token operator">-></span> r<span class="token punctuation">.</span><span class="token function">key</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toMap</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token string">"id"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                r <span class="token operator">-></span> r<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toObject</span><span class="token punctuation">(</span><span class="token class-name">Customer</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token class-name">JobConfig</span> cfg <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">JobConfig</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setName</span><span class="token punctuation">(</span><span class="token string">"mysql-monitor"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">Jet</span><span class="token punctuation">.</span><span class="token function">bootstrappedInstance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">newJob</span><span class="token punctuation">(</span>pipeline<span class="token punctuation">,</span> cfg<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="architecture"></a><a href="#architecture" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Architecture</h2>
<p>I have stated above that when Debezium is integrated into Jet, the
latter takes on the role of service-provider as far as fault tolerance
and scalability are concerned.</p>
<p>Jet doesn't delegate its cluster management and fault tolerance concerns
to an outside system like ZooKeeper. It reuses the groundwork
implemented for Hazelcast IMDG: cluster management and the IMap, and
adds its own implementation of Chandy-Lamport distributed snapshots. If
a cluster member fails, Jet will restart the job on the remaining
members, restore the state of processing from the last snapshot, and
then seamlessly continue from that point. For further details, consult
our <a href="https://jet-start.sh/docs/next/architecture/fault-tolerance">documentation on the
topic</a>.</p>
<p>Extending this functionality umbrella to cover Debezium has been
surprisingly simple. All we had to do was to add Debezium’s
source offset to Jet’s snapshots. This way, whenever Jet needs to
execute a recovery, it passes the recovered offset to Debezium,
which in turn resumes the data flow from that offset.</p>
<p>One other thing we did and might be worth mentioning is that the Jet
integration also makes use of Debezium’s <a href="https://debezium.io/documentation/reference/1.2/configuration/event-flattening.html">new record state
extraction</a>
SMT (Simple Message Transformation), for the purpose of message
structure simplification. With this transformation in effect, only the
&quot;after&quot; structure of the Debezium event envelope is processed by Jet.
However, whether this is a good idea or not, only time will tell. I
personally think that if and when we will start covering schema changes
more, we might end up re-enabling the full Debezium event content.</p>
<h2><a class="anchor" aria-hidden="true" id="examples"></a><a href="#examples" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Examples</h2>
<p>The simplest example of using the Jet-Debezium integration would be our
<a href="https://jet-start.sh/docs/next/tutorials/cdc">CDC tutorial</a> that I’ve
already mentioned above. A more involved one can be seen in my
colleague’s, Nicolas Fränkel’s <a href="https://jet-start.sh/blog/2020/07/16/designing-evergreen-cache-cdc">blog
post</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="license"></a><a href="#license" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>License</h2>
<p>The Jet - Debezium integration is currently provided under the <a href="https://www.apache.org/licenses/LICENSE-2.0.txt">Apache
License, Version 2</a>,
just like Debezium and most of Jet, so making full usage of the
combination of the two should have no impediments in your own projects.</p>
<h2><a class="anchor" aria-hidden="true" id="looking-ahead"></a><a href="#looking-ahead" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Looking ahead</h2>
<p>At the moment of writing the Jet-Debezium integration is fully finished
only for MySQL and Postgres databases and has been <a href="https://jet-start.sh/blog/2020/07/14/jet-42-is-released">released in version
4.2</a> of Jet.
Further work on covering more connectors and extending current
ones (for example by adding handling for database schema changes),
has not yet been scheduled.</p>
<p>The functionality provided by Debezium, the ability to allow modern
processing of legacy data is a great fit to Jet’s ability to carry out
that processing efficiently. The combination of the two has the
potential to become much more than the sum of their parts. I very much
look forward to finding out what this integration can lead to. Stay
tuned!</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/08/05/gc-tuning-for-jet">Sub-10 ms Latency in Java: Concurrent GC with Green Threads</a></h1><p class="post-meta">August 5, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener">Marko Topolnik</a></p><div class="authorPhoto"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener"><img src="https://i.imgur.com/xuavzce.jpg" alt="Marko Topolnik"/></a></div></div></header><article class="post-content"><div><span><p>This post is a part of a series:</p>
<ul>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Part 1 (Intro and high-throughput streaming
benchmark)</a></li>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part 2 (batch workload benchmark)</a></li>
<li><a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Part 3 (low-latency benchmark)</a></li>
<li>Part 4 (you are here)</li>
</ul>
<p>In Part 3 we showed that a modern JVM running live stream aggregation
can achieve a 99.99% latency lower than 10 milliseconds. The focus of
that post was comparing the different GC options available for the JVM.
In order to maintain a level playing field, we kept to the default
settings as much possible.</p>
<p>In this round we wanted to look at the same problem from the opposite
angle: what can we do to help Hazelcast Jet achieve the best performance
available on a JVM? How much throughput can we get while staying within
the tight 10 ms bound for 99.99th percentile latency? We found our
opportunity in a distinct design feature of Jet: the Cooperative Thread
Pool.</p>
<h2><a class="anchor" aria-hidden="true" id="native-threads-with-concurrent-gc"></a><a href="#native-threads-with-concurrent-gc" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Native Threads with Concurrent GC</h2>
<p>Let's go through an example with a streaming job running on a four-core
machine. In a typical execution engine design, every task (roughly
corresponding to a <a href="/docs/concepts/dag">DAG vertex</a>) gets its own thread
to execute it:</p>
<p><img src="/blog/assets/2020-08-05-dag1.svg" alt="Native Multithreading"></p>
<p>There are eight threads and the OS is in charge of deciding how to
schedule them to run on the four available cores. The application has
no direct control over this and the cost of switching from one thread
to another on the same CPU core is around 2-10 microseconds.</p>
<p>This is how it will look when we add a concurrent GC thread into the
picture:</p>
<p><img src="/blog/assets/2020-08-05-dag1-with-gc.svg" alt="Native Multithreading with a GC Thread"></p>
<p>There's one more thread now, the concurrent GC thread, and it's
additionally interfering with the computation pipeline.</p>
<h2><a class="anchor" aria-hidden="true" id="green-threads-with-concurrent-gc"></a><a href="#green-threads-with-concurrent-gc" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Green Threads with Concurrent GC</h2>
<p>In Hazelcast Jet, tasks are designed to be
<a href="/docs/architecture/execution-engine">cooperative</a>: every time you give
it a bunch of data to process, the task will run for a short while and
return. It doesn't have to process all the data in one go and the
execution engine will give it control again later with all the
still-pending data. This basic design is also present in the concepts of
<em>green threads</em> and <em>coroutines</em>. In Hazelcast Jet we call them
<a href="/docs/architecture/execution-engine#tasklet"><em>tasklets</em></a>.</p>
<p>This design allows Jet to always use the same, fixed-size thread pool no
matter how many concurrent tasks it instantiates to run a data pipeline.
So, on the example of a four-core machine, it looks like this:</p>
<p><img src="/blog/assets/2020-08-05-dag2.svg" alt="Cooperative Multithreading"></p>
<p>By default, Jet creates as many threads for itself as there are
available CPU cores, and inside each thread there are many tasklets.
Switching from one tasklet to the next is extremely cheap — it
boils down to one tasklet returning from its <code>call()</code> method, the
top-level loop taking the next tasklet from a list, and invoking its
<code>call()</code> method. If you wonder at this point what happens to blocking IO
calls, for example connecting to a JDBC data source, Jet does support a
backdoor where it creates a dedicated thread for such a tasklet. Threads
that block for IO aren't CPU-bound and usually their interference is
quite low, but in a low-latency applications you should avoid depending
on blocking APIs.</p>
<p>Now comes another advantage of this design: if we know there will also
be a concurrent GC thread, we can configure Jet to use one thread less:</p>
<p><img src="/blog/assets/2020-08-05-dag2-with-gc.svg" alt="Cooperative Multithreading with a GC Thread"></p>
<p>There are still as many threads as CPU cores and the OS doesn't have to
do any context switching. We did give up one entire CPU core just for
GC, reducing the CPU capacity available to Jet, but we allowed
background GC to run truly concurrently to the Jet tasks. In low-latency
scenarios, <em>the application doesn't need 100% CPU, but it needs its
share of the CPU 100% of the time.</em></p>
<p>We went to see if this setup really makes the difference we hope for,
and found it indeed had a drammatic impact on the latency with both
garbage collectors we tested (G1 and ZGC). The most important outcome
was that we were now able to push G1 below the 10 ms line. Since G1 is
stable across a wide range of throughputs, we immediately got it to
perform within 10 ms at <em>double the throughput than in the previous
round</em>.</p>
<h2><a class="anchor" aria-hidden="true" id="the-setup"></a><a href="#the-setup" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Setup</h2>
<p>Based on the expectations set by the previous benchmark, we focused on
the ZGC and G1 collectors and the latest pre-release of Java 15. Our
setup stayed the same for the most part; we refreshed the code a bit and
now use the released version 4.2 of Hazelcast Jet with OpenJDK 15 EA33.</p>
<p>We also implemented a parallelized event source simulator. Its higher
throughput capacity allows it to catch up faster after a hiccup, helping
to reduce the latency a bit more. The processing pipeline itself is
identical to the previous round,
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/round-3/src/main/java/org/example/StreamingRound3.java">here</a>
is the complete source code.</p>
<p>We determined how many threads the given GC uses, set the size of the
Jet thread pool to 16
(<a href="https://aws.amazon.com/ec2/instance-types/c5/">c5.4xlarge</a> vCPUs)
minus that value and then did some trial-and-error runs to find the
optimum. G1 uses 3 threads, so we gave Jet 13. ZGC uses just 2 threads,
but we found Jet to perform a bit better with 13 instead of the
theoretical 14 threads, so we used that. We also experimented with
changing the GC's automatic choice for the thread count, but didn't find
a setting that would beat the default.</p>
<p>Additionally, with G1 we saw that in certain cases, even with
<code>MaxGCPauseMillis=5</code> (same as in the previous post), the size of the new
generation would grow large enough for Minor GC pauses to impact
latency. Therefore we added <code>MaxNewSize</code> with one of <code>100m</code>, <code>150m</code> and
<code>200m</code>, depending on the chosen throughput. This was also determined
through trial and error, the results seemed to be the best when a minor
GC was occurring about 10-20 times per second.</p>
<p>Summarizing, these are the changes we made with respect to the setup in
the previous post:</p>
<ol>
<li>Reduced Jet's cooperative thread pool size</li>
<li>Parallel event source where previously it was single-threaded</li>
<li>Used the <code>MaxNewSize</code> JVM parameter for G1</li>
<li>Updated Hazelcast Jet and JDK versions</li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="the-results"></a><a href="#the-results" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Results</h2>
<p>Comparing ZGC's results below with those in the <a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch#a-sneak-peek-into-upcoming-versions">previous
round</a>,
we can see the latency stayed about the same where it was already good,
but the range of throughputs got extended from 8 to 10 M items/second,
a solid 25% improvement.</p>
<p>The effect on G1 is sort of dual to the above: while the G1 already had
great throughput but fell just short of making it below the 10 ms line,
in this round its latency improved across the board, up to 40% at
places. The best news: <em>the maximum throughput at which a single
Hazelcast Jet node maintains 99.99% latency within 10 ms now lies at 20
million items per second</em>, a 250% boost!</p>
<p><img src="/blog/assets/2020-08-05-latency-1m.png" alt="Latency on c5.4xlarge, 1 M Events per Second"></p>
<h2><a class="anchor" aria-hidden="true" id="upgrading-to-10-m-input-events-per-second"></a><a href="#upgrading-to-10-m-input-events-per-second" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Upgrading to 10 M Input Events per Second</h2>
<p>Encouraged by this strong result, we dreamed up a scenario like this: we
have 100,000 sensors, each producing a 100 Hz measurement stream. Can a
single-node Hazelcast Jet handle this load and produce, say, the time
integral of the measured quantity from each sensor over a 1-second
window, at a 10 ms latency? This implies an order-of-magnitude leap in
the event rate, from 1 M to 10 M events per second, but also a reduction
in window length by the same factor, from ten seconds to one.</p>
<p>Nominally, the scenario results in the same combined input+output
throughput as well as about the same size of state that we already saw
work: 20 M items/second and 10 M stored map entries. It's the maximum
point where G1 was still inside 10 ms, but even at 25 M items/second it
still had pretty low latency. However, for reasons we haven't yet
identified, the input rate seems to have a stronger impact on GC, so
when we traded output for input, it turned out that G1 was nowhere near
handling it.</p>
<p>But, since we picked the c5.4xlarge instance type as a medium-level
option, for this &quot;elite scenario&quot; we considered the top-shelf EC2 box as
well: c5.metal. It commands 96 vCPUs and has some scary amount of RAM
that we won't need. On this hardware G1 decides to take 16 threads for
itself, so the natural choice would be 80 threads for Jet. However,
through trial and error we chased down the real optimum, which turned
out to be 64 threads. Here is what we got:</p>
<p><img src="/blog/assets/2020-08-05-latency-10m.png" alt="Latency on c5.metal, 10 M Events per Second"></p>
<p>G1 comfortably makes it to the 20 M mark and then goes on all the way to
40 M items per second, gracefully degrading and reaching 60 M with just
12 ms. Beyond this point it was Jet who ran out of steam. The Jet
pipeline running at full speed just couldn't max out the G1! We repeated
the test with more threads given to Jet, 78, but that didn't make a
difference.</p>
<p><em>If you enjoyed reading this post, check out Jet at
<a href="https://github.com/hazelcast/hazelcast-jet">GitHub</a> and give us a
star!</em></p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/07/16/designing-evergreen-cache-cdc">Designing an Evergreen Cache with Change Data Capture</a></h1><p class="post-meta">July 16, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/nicolas_frankel" target="_blank" rel="noreferrer noopener">Nicolas Frankel</a></p><div class="authorPhoto"><a href="https://twitter.com/nicolas_frankel" target="_blank" rel="noreferrer noopener"><img src="https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2019/12/nicolas-frankel-170x170.jpg" alt="Nicolas Frankel"/></a></div></div></header><article class="post-content"><div><span><p>It has been said that there are two things hard in software development:
naming things and cache invalidation (while some add off-by-one errors
to the mix).
I believe that keeping the cache in sync with the source of truth might
count as a third one.
In this post, I'd like to tackle this issue, describe the ideal
situation -
1 cache, 1 datastore - describe the problem of having multiple components
that can write to the datastore, list all possible solutions, and
describe one elegant solution based on Change Data Capture and Jet.</p>
<h2><a class="anchor" aria-hidden="true" id="the-ideal-design"></a><a href="#the-ideal-design" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The ideal design</h2>
<p>In a system, to improve performance, one of the first short-term
measures is to set up a cache.
It's a tradeoff between getting the data faster at the cost of the data
being not that fresh: one loads the data in-memory close to the
consumer, and presto, one gets an instant performance boost. In regard
to a database, this is akin to the following:</p>
<p><img src="/blog/assets/2020-07-16-starting-architecture.svg" alt="Starting architecture"></p>
<p>In this read-through design, when the app requires an item, the cache
first checks whether it has it.
If yes, it returns it.
If not, it fetches it from the underlying Relational Database Management
System, stores it, and returns it.
For a write, it stores it, and also calls the RDBMS to store it.</p>
<p>Note that using a cache-aside design instead of read-through would have
the same issue.
The only difference would be the fact that the app would be responsible
for the fetching/storing logic instead of the cache.</p>
<p>The RDBMS is the sole source of truth - as it should be. Since the cache
intercepts write statements to the RDBMS, it's a mirror of the data.</p>
<h2><a class="anchor" aria-hidden="true" id="handling-third-party-database-updates"></a><a href="#handling-third-party-database-updates" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Handling third-party database updates</h2>
<p>This design works as expected as long as the database doesn't receive
updates from another source:</p>
<p><img src="/blog/assets/2020-07-16-updating-database-bypassing-cache.svg" alt="Updating the database while bypassing the
cache"></p>
<p>Now, the RDBMS is still the source of truth, but the cache is not aware
of changes made by other components.
Hence, it might (will) return data that it has stored, but that is stale
compared to what is the source of truth in the RDBMS.</p>
<p>There are multiple ways to cope with this issue.</p>
<h3><a class="anchor" aria-hidden="true" id="cache-invalidation"></a><a href="#cache-invalidation" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Cache invalidation</h3>
<p>Since the cache only queries the RDBMS if it doesn't store the requested
item, let's remove items after a specific time.
This is a built-in feature in enterprise-grade caches such as Hazelcast
IMDG and it is known as the Time-To-Live.
When an item is stored, a TTL can be attached to it.
After that time has elapsed, the item is removed from the cache and it
will be fetched from the RDBMS again if needed.</p>
<p>This approach has a couple of downsides:</p>
<ol>
<li><p>If an item is not updated in the RDBMS, but is evicted from the
cache, then there's an extra query from the cache to the RDBMS when
it's needed by the app. This is a net loss of resources.</p></li>
<li><p>If an item is updated in the RDBMS, but its TTL has not been reached
yet, then the cache will return the stale data. This defeats the
purpose.</p></li>
</ol>
<p>With longer TTL, we avoid unnecessary round trips but return more stale
data.
With shorter TTL, we waste resources with lesser chances of stale data.</p>
<h3><a class="anchor" aria-hidden="true" id="polling-the-rdbms"></a><a href="#polling-the-rdbms" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Polling the RDBMS</h3>
<p>Because the TTL doesn't seem to be the right approach, we could devise a
dedicated component that watches the RDBMS by regularly sending queries
to it and updating the cache accordingly.</p>
<p>Unfortunately, this strategy incurs the same issues as cache
invalidation:
the more frequent the queries, the more chances to catch changes, but
the more resources are wasted.
Worse, this also will put extra load on the RDBMS.</p>
<h3><a class="anchor" aria-hidden="true" id="rdbms-triggers"></a><a href="#rdbms-triggers" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>RDBMS triggers</h3>
<p>A common downside of the above approaches is the way they both poll the
database.
Polling happens with a specific frequency, while writes don't follow any
regular periodicity.
Thus, it's not possible to make the two match.</p>
<p>Instead of polling, it would make much more sense to be event-driven:</p>
<ol>
<li><p>if no writes happen, there's no need to update the cache</p></li>
<li><p>if a write happens, then the relevant cache item should be updated
accordingly</p></li>
</ol>
<p>In RDBMS this event-driven approach is implemented via <em>triggers</em>.
Triggers are dedicated stored procedures that are launched in response
to specific events, such as an <code>INSERT</code> or an <code>UPDATE</code>.</p>
<p>That works pretty well when the acted-upon object is inside the database,
e.g. &quot;when a record of table A is updated, then add a record to table
B&quot;.
For our use case where the acted-upon object is the cache which sits
outside the database, it's not as simple.
For example, MySQL allows you to <a href="https://dev.mysql.com/doc/refman/8.0/en/faqs-triggers.html#faq-mysql-can-triggers-udf">make an external system call from a
trigger</a>.
However, this approach is very implementation-dependent and makes the
overall design of the system much more fragile.
Also, only some RDBMS implement triggers. Even if they do, there's no
standard implementation.</p>
<h2><a class="anchor" aria-hidden="true" id="change-data-capture"></a><a href="#change-data-capture" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Change Data Capture</h2>
<p>Wikipedia defines Change Data Capture (or CDC) as:</p>
<blockquote>
<p>[...] a set of software design patterns used to determine and track
the data that has changed so that action can be taken using the
changed data.</p>
<p>CDC is an approach to data integration that is based on the
identification, capture and delivery of the changes made to enterprise
data sources.</p>
</blockquote>
<p>In practice, CDC is a tool that allows to transform standard write
queries into events.
It implements it by &quot;turning the database inside-out&quot; (quote from Martin
Kleppmann).
This definition is because a database keeps a record of all changes in
an implementation-dependent append-only log.
Regularly, it uses it to manage its state. Some RDBMS also have other
usage, e.g. MySQL uses the log for replication across nodes.</p>
<p>For example, here's a sample for MySQL binlog:</p>
<pre><code class="hljs css language-text">### UPDATE `test`.`t`
### WHERE
###   @1=1 /* INT meta=0 nullable=0 is_null=0 */
###   @2='apple' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */
###   @3=NULL /* VARSTRING(20) meta=0 nullable=1 is_null=1 */
### SET
###   @1=1 /* INT meta=0 nullable=0 is_null=0 */
###   @2='pear' /* VARSTRING(20) meta=20 nullable=0 is_null=0 */
###   @3='2009:01:01' /* DATE meta=0 nullable=1 is_null=0 */
# at 569
#150112 21:40:14 server id 1  end_log_pos 617 CRC32 0xf134ad89
#Table_map: `test`.`t` mapped to number 251
# at 617
#150112 21:40:14 server id 1  end_log_pos 665 CRC32 0x87047106
#Delete_rows: table id 251 flags: STMT_END_F
</code></pre>
<p>A CDC component connects to this immutable log to extract change events.</p>
<p>One can view CDC as the opposite of Event Sourcing:
the latter captures state by aggregating events, while the former
extracts events &quot;from the state&quot;.</p>
<h2><a class="anchor" aria-hidden="true" id="debezium"></a><a href="#debezium" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Debezium</h2>
<p>CDC is quite recent and hasn't had time to mature.
As such, there's no universal standard, but specific tools.
In this section, we are going to have a look at
<a href="https://debezium.io/">Debezium</a>.
Debezium is an Open Source set of services for CDC provided by Red Hat.</p>
<p>Debezium is an umbrella term covering several components:</p>
<ol>
<li><p>Debezium Connectors are specific bridges that read the append-only
proprietary log for each supported database. For example, there’s a
connector for MySQL, one for MongoDB, one for PostgreSQL, etc.</p></li>
<li><p>Each connector is also a Kafka Connect Source Connector:
this allows to easily output CDC events to one’s Kafka cluster</p></li>
<li><p>Finally, the Debezium Engine is a JAR that allows Debezium to be
embedded in one’s applications. Note that even in that case, Debezium
produces Kafka Connect-specific content, which then needs to be
handled and transformed in one’s application.</p></li>
</ol>
<p>While Kafka is a great technology and probably also quite widespread
nowadays, data in Kafka needs to be persisted to disk.
The benefit of persistence is that data survive even in the event of the
cluster going down.
The tradeoff, however, is that the access time of disk-persisted data is
one (or 2) orders of magnitude slower than the access time of in-memory
data, depending on the underlying disk technology.</p>
<h2><a class="anchor" aria-hidden="true" id="hazelcast-jet"></a><a href="#hazelcast-jet" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Hazelcast Jet</h2>
<p><a href="https://jet-start.sh/">Hazelcast Jet</a> is a distributed stream
processing framework built on Hazelcast and combines a cache with
fault-tolerant data processing.
It has sources and sinks to integrate with various file, messaging and
database systems (such as Amazon S3, Kafka, message brokers and
relational databases).</p>
<p>Jet also provides a Debezium module where it can process change events
directly from the database and write them to its distributed key-value
store.
This avoids having to write the intermediate messages to Kafka and then
read again to be written to a separate cache.</p>
<h2><a class="anchor" aria-hidden="true" id="putting-it-all-together"></a><a href="#putting-it-all-together" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Putting it all together</h2>
<p>It’s (finally!) time to assemble all the previous bits together.
Here are the components and their responsibilities:</p>
<ol>
<li><p>A MySQL database instance is where the data is stored.
It’s accessed in read-only mode by the cache, and in write-only mode
by some external component</p></li>
<li><p>A Jet instance reads events from MySQL through the Debezium connector,
transforms them into cache-compatible key-value pairs, and updates
the cache accordingly. Note that while Jet pipelines provide
filtering capabilities, it’s also possible to filter items in the CDC
connector to optimize the load of the pipeline</p></li>
<li><p>The app uses the cache, which is always up-to-date with the database,
give or take the time it takes for the above to execute</p></li>
</ol>
<p><img src="/blog/assets/2020-07-16-architecture-with-cdc.svg" alt="Final architecture with
CDC"></p>
<p>Note that this architecture assumes one starts from a legacy state with
an existing app that uses caching, where a new component that could
update the database was set up later on.</p>
<p>If one starts from scratch, it’s possible to simplify the above diagram
(and associated code) as Jet embeds its own Hazelcast instance.
In that case instead of Jet being a client of a third-party Hazelcast
instance, Jet is the one to configure and start the instance.
Obviously, it also can then get/put data.</p>
<h2><a class="anchor" aria-hidden="true" id="talk-is-cheap-show-me-the-code"></a><a href="#talk-is-cheap-show-me-the-code" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Talk is cheap, show me the code</h2>
<p>Sources for this post are available <a href="https://github.com/hazelcast-demos/evergreen-cache">on
GitHub</a>.</p>
<p>The repository is made of the following modules:</p>
<ul>
<li><p><code>app</code> is a Spring Boot application using Spring Data JDBC to access a
MySQL database. It abstracts away Hazelcast by using a Spring Cache
layer</p></li>
<li><p><code>update</code> is a Spring Shell application.
It allows to update the data inside the database, with the cache none
the wiser</p></li>
<li><p><code>pipeline</code> is the Jet pipeline that listens to CDC events and updates
the cache when data is updated</p></li>
</ul>
<p>The pipeline definition is quite straightforward:</p>
<pre><code class="hljs css language-java">pipeline<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span>source<span class="token punctuation">)</span>                                       <span class="token comment">//1</span>
        <span class="token punctuation">.</span><span class="token function">withoutTimestamps</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">CdcSinks</span><span class="token punctuation">.</span><span class="token function">remoteMap</span><span class="token punctuation">(</span>                            <span class="token comment">//2</span>
                <span class="token string">"entities"</span><span class="token punctuation">,</span>                                     <span class="token comment">//3</span>
                <span class="token keyword">new</span> <span class="token class-name">CustomClientConfig</span><span class="token punctuation">(</span>env<span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token string">"CACHE_HOST"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment">//4</span>
                r <span class="token operator">-></span> r<span class="token punctuation">.</span><span class="token function">key</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toMap</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token string">"id"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                 <span class="token comment">//5</span>
                r <span class="token operator">-></span> r<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toObject</span><span class="token punctuation">(</span><span class="token class-name">Person</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span>           <span class="token comment">//6</span>
        <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<ol>
<li><p>Get a stream of Jet <code>ChangeRecord</code> items</p></li>
<li><p>Create the sink to write to a remote map</p></li>
<li><p>Name of the remote map</p></li>
<li><p>Client configuration so it can connect to the right host, cluster
and instance</p></li>
<li><p>Provide a mapping function to extract the cache key from the
<code>ChangeRecord</code></p></li>
<li><p>Provide a mapping function to extract the cache value (<code>Person</code> POJO)
from the <code>ChangeRecord</code></p></li>
</ol>
<pre><code class="hljs css language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">CustomClientConfig</span> <span class="token keyword">extends</span> <span class="token class-name">ClientConfig</span> <span class="token punctuation">{</span>

  <span class="token keyword">public</span> <span class="token class-name">CustomClientConfig</span><span class="token punctuation">(</span><span class="token class-name">String</span> cacheHost<span class="token punctuation">)</span> <span class="token punctuation">{</span>
    <span class="token function">getNetworkConfig</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">addAddress</span><span class="token punctuation">(</span>cacheHost <span class="token operator">!=</span> <span class="token keyword">null</span> <span class="token operator">?</span> cacheHost <span class="token operator">:</span> <span class="token string">"localhost"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre>
<p>To try the demo, a local Docker <em>daemon</em> must be running.</p>
<ol>
<li><p>To create the necessary Docker images, at the root of the project,
run the build:</p>
<pre><code class="hljs css language-bash">mvn compile
</code></pre></li>
<li><p>At the root of the repo, run the compose file.
This will start a MySQL instance, the app, the Jet pipeline job, as
well as Hazelcast Management Center to get additional insight into
the cluster state</p>
<pre><code class="hljs css language-bash">docker-compose up
</code></pre></li>
<li><p>Open a browser at <a href="http://localhost:8080/">http://localhost:8080/</a></p></li>
<li><p>Refresh the browser, and check the logs: there should be no
interaction with the database, only with the cache</p></li>
<li><p>In the <code>update</code> module, set the database user and password and then
execute the Maven Spring Boot plugin:</p>
<pre><code class="hljs css language-bash"><span class="hljs-built_in">export</span> SPRING_DATASOURCE_USERNAME=root
<span class="hljs-built_in">export</span> SPRING_DATASOURCE_PASSWORD=root
mvn spring-boot:run
</code></pre>
<p>This will open an interactive shell to execute specific commands.
The update command requires two arguments, the primary key of the
<code>Person</code> entity to update, and the new value for the <code>firstName</code> column.
The following command will update the <code>firstName</code> value of the
entity with PK <code>1</code> with value <code>&quot;Foo&quot;</code></p>
<pre><code class="hljs css language-bash">update 1 Foo
</code></pre></li>
<li><p>Refresh the browser again. The cache has been updated, the
application doesn’t access the database, and still the value <code>Foo</code>
should be shown for entity with PK <code>1</code></p></li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="conclusion"></a><a href="#conclusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>Caching is easy if the cache is the only component that writes to the
database.
As soon as other components update the database, no traditional strategy
to keep the data of the database and the cache in sync is satisfying.</p>
<p>The Hazelcast Jet streaming engine, using Debezium under the hood, is
able to leverage Change Data Capture over traditional RDBMS to achieve
an evergreen cache in a simple way.</p>
<h2><a class="anchor" aria-hidden="true" id="references"></a><a href="#references" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>References</h2>
<ul>
<li><p><a href="https://jet-start.sh/docs/get-started/intro">Introduction to Jet</a></p></li>
<li><p><a href="https://jet-start.sh/docs/tutorials/cdc">Change Data Capture from MySQL</a></p></li>
<li><p><a href="https://debezium.io/blog/2020/02/10/event-sourcing-vs-cdc/">Event Sourcing vs. Change Data
Capture</a></p></li>
<li><p><a href="https://www.percona.com/blog/2015/01/20/identifying-useful-information-mysql-row-based-binary-logs/">Identifying useful info from MySQL row-based binary
logs</a></p></li>
<li><p><a href="https://github.com/hazelcast-demos/evergreen-cache">Demo source code</a></p></li>
</ul>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/07/14/jet-42-is-released">Jet 4.2 is Released</a></h1><p class="post-meta">July 14, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/cgencer" target="_blank" rel="noreferrer noopener">Can Gencer</a></p><div class="authorPhoto"><a href="http://twitter.com/cgencer" target="_blank" rel="noreferrer noopener"><img src="https://pbs.twimg.com/profile_images/1187734846749196288/elqWdrPj_400x400.jpg" alt="Can Gencer"/></a></div></div></header><article class="post-content"><div><span><p>Jet 4.2 is finally here! Here's an overview of what's new:</p>
<h2><a class="anchor" aria-hidden="true" id="change-data-capture-support-for-mysql-and-postgresql"></a><a href="#change-data-capture-support-for-mysql-and-postgresql" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Change Data Capture Support for MySQL and PostgreSQL</h2>
<p>Previously, Jet has had support for <a href="https://debezium.io/">Debezium</a> as
a contrib package. We're happy to announce that we've made several
improvements to this package and decided to make this a part of our main
release.</p>
<p>Debezium was developed initially as a Kafka Connect module, which can
read the snapshot and changes of relational databases such as MySQL and
PostgreSQL. Jet's Debezium integration removes the Kafka dependency
completely, and you can work with the stream of changes directly using
the full power of the Jet API.</p>
<p>Along with this change, we've created a new high-level API which makes
it easier to work directly with change stream records. For example, to
observe changes from MySQL, all you need to do do is:</p>
<pre><code class="hljs css language-java">pipeline<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span>
    <span class="token class-name">MySqlCdcSources</span><span class="token punctuation">.</span><span class="token function">mysql</span><span class="token punctuation">(</span><span class="token string">"customers"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setDatabaseAddress</span><span class="token punctuation">(</span><span class="token string">"127.0.0.1"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setDatabasePort</span><span class="token punctuation">(</span><span class="token number">3306</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setDatabaseUser</span><span class="token punctuation">(</span><span class="token string">"debezium"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setDatabasePassword</span><span class="token punctuation">(</span><span class="token string">"dbz"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setClusterName</span><span class="token punctuation">(</span><span class="token string">"dbserver1"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setDatabaseWhitelist</span><span class="token punctuation">(</span><span class="token string">"inventory"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">setTableWhitelist</span><span class="token punctuation">(</span><span class="token string">"inventory.customers"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">withNativeTimestamps</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>You can also combine this feature with Jet's
<a href="/docs/api/data-structures">in-memory-storage</a> allowing you to build an
in-memory replica of the database in just a few lines of code. The
example below will hydrate the distributed map <code>customers</code> with the
records from the database table with the same name:</p>
<pre><code class="hljs css language-java"><span class="token class-name">StreamSource</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">ChangeRecord</span><span class="token punctuation">></span></span> source <span class="token operator">=</span> <span class="token class-name">MySqlCdcSources</span><span class="token punctuation">.</span><span class="token function">mysql</span><span class="token punctuation">(</span><span class="token string">"source"</span><span class="token punctuation">)</span>
   <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
    <span class="token punctuation">.</span><span class="token function">setTableWhitelist</span><span class="token punctuation">(</span><span class="token string">"inventory.customers"</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

pipeline<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span>source<span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">withoutTimestamps</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">peek</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">CdcSinks</span><span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token string">"customers"</span><span class="token punctuation">,</span>
                r <span class="token operator">-></span> r<span class="token punctuation">.</span><span class="token function">key</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toMap</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">get</span><span class="token punctuation">(</span><span class="token string">"id"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                r <span class="token operator">-></span> r<span class="token punctuation">.</span><span class="token function">value</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toObject</span><span class="token punctuation">(</span><span class="token class-name">Customer</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>For a more in-depth example of this feature, see the <a href="/docs/tutorials/cdc">CDC
Tutorials</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="elasticsearch-connectors"></a><a href="#elasticsearch-connectors" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>ElasticSearch Connectors</h2>
<p>We've also had ElasticSearch (5, 6, 7) connectors available as contrib
modules and happy to announce that they have also been through several
rounds of improvements and merged into the main release. A summary is
below:</p>
<ul>
<li>Support for slicing reads: Jet can use the slicing feature of
ElasticSearch to read data in parallel.</li>
<li>Collocated read and write: You can make use of collocated reading from
ElasticSearch by placing Jet on the same nodes as your ElasticSearch
cluster - this will significantly improve the speed of querying and
ingestion.</li>
<li>Improved the retry mechanism for writes: As Jet can be used to write
to ElasticSearch as part of a streaming job, we've improved the retry
mechanism so that transient ES cluster failures can be retries.</li>
</ul>
<p>The ElasticSearch source and sink can be used with a simple API with the
example given below:</p>
<pre><code class="hljs css language-java"><span class="token class-name">BatchSource</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">></span></span> elasticSource <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">ElasticSourceBuilder</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">name</span><span class="token punctuation">(</span><span class="token string">"elastic-source"</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">clientFn</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token class-name">RestClient</span><span class="token punctuation">.</span><span class="token function">builder</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">HttpHost</span><span class="token punctuation">(</span>
                <span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">9200</span>
        <span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">searchRequestFn</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token keyword">new</span> <span class="token class-name">SearchRequest</span><span class="token punctuation">(</span><span class="token string">"my-index"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">optionsFn</span><span class="token punctuation">(</span>request <span class="token operator">-></span> <span class="token class-name">RequestOptions</span><span class="token punctuation">.</span>DEFAULT<span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">mapToItemFn</span><span class="token punctuation">(</span>hit <span class="token operator">-></span> hit<span class="token punctuation">.</span><span class="token function">getSourceAsString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">slicing</span><span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token class-name">Sink</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Map</span><span class="token punctuation">&lt;</span><span class="token class-name">String</span><span class="token punctuation">,</span> <span class="token class-name">Object</span><span class="token punctuation">></span><span class="token punctuation">></span></span> elasticSink <span class="token operator">=</span> <span class="token class-name">ElasticSinks</span><span class="token punctuation">.</span><span class="token function">elasticsearch</span><span class="token punctuation">(</span>
    <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token function">client</span><span class="token punctuation">(</span><span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"password"</span><span class="token punctuation">,</span> <span class="token string">"host"</span><span class="token punctuation">,</span> <span class="token number">9200</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    item <span class="token operator">-></span> <span class="token keyword">new</span> <span class="token class-name">IndexRequest</span><span class="token punctuation">(</span><span class="token string">"my-index"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">source</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>See
<a href="https://github.com/hazelcast/hazelcast-jet/tree/master/examples/elastic">GitHub</a>
for a full, end-to-end example.</p>
<h2><a class="anchor" aria-hidden="true" id="rebalance-operator"></a><a href="#rebalance-operator" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>.rebalance() operator</h2>
<p>Hazelcast Jet, by default, prefers not to send the data around the
computing cluster. If your data source retrieves some part of the data
stream on member A and you apply a mapping to it, the processing will
only happen on member A. If you have a non-distributed data source, this
may mean that all processing only happens on one member.</p>
<p>Jet 4.2 introduces the <code>.rebalance()</code> operator, which lets Jet
re-distribute data across the cluster at any point in the pipeline. To
use it is very simple:</p>
<pre><code class="hljs css language-java"><span class="token class-name">Pipeline</span> p <span class="token operator">=</span> <span class="token class-name">Pipeline</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">Sources</span><span class="token punctuation">.</span><span class="token function">itemStream</span><span class="token punctuation">(</span><span class="token number">1_000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">withIngestionTimestamps</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
</code></pre>
<p>For more details, please see the <a href="/docs/api/more-transforms#rebalance">documentation page</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="improved-json-support"></a><a href="#improved-json-support" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Improved JSON Support</h2>
<p>In previous versions of Jet, it was possible to read JSON files using
the file source, but it required some manual effort to set up the parsing
yourself. With 4.2, we're now making use of
<a href="https://github.com/FasterXML/jackson-jr">jackson-jr</a> to parse JSON
files and offer a native JSON file source. The source provides support
for object mapping out of the box, so all you need to do is like below:</p>
<pre><code class="hljs css language-java"><span class="token class-name">Pipeline</span> p <span class="token operator">=</span> <span class="token class-name">Pipeline</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">Sources</span><span class="token punctuation">.</span><span class="token function">json</span><span class="token punctuation">(</span><span class="token string">"/home/data/people"</span><span class="token punctuation">,</span> <span class="token class-name">Person</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>person <span class="token operator">-></span> person<span class="token punctuation">.</span><span class="token function">location</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">equals</span><span class="token punctuation">(</span><span class="token string">"NYC"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>Similarly, we have added a sink that can output JSON files
in the same way. For more details, please see the <a href="/docs/api/sources-sinks#json-files">documentation
page</a>.</p>
<h3><a class="anchor" aria-hidden="true" id="support-for-json-parsing"></a><a href="#support-for-json-parsing" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Support for JSON parsing</h3>
<p>As part of JSON improvements, there is also now a built-in utility
method which you can use to parse JSON inside a pipeline:</p>
<pre><code class="hljs css language-java">stage<span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>json <span class="token operator">-></span> <span class="token class-name">JsonUtil</span><span class="token punctuation">.</span><span class="token function">beanFrom</span><span class="token punctuation">(</span>json<span class="token punctuation">,</span> <span class="token class-name">Person</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>See the <a href="/docs/api/more-transforms#json">documentation page</a> for
additional instructions.</p>
<h2><a class="anchor" aria-hidden="true" id="apache-pulsar-connector"></a><a href="#apache-pulsar-connector" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Apache Pulsar Connector</h2>
<p><a href="https://pulsar.apache.org/">Apache Pulsar</a> is a popular, fault-tolerant
pub-sub messaging system which is a good fit for stream processing
systems. A connector for Apache Pulsar is now also available as a
<a href="https://github.com/hazelcast/hazelcast-jet-contrib/tree/master/pulsar">contrib module</a>
.</p>
<p>The source is fault-tolerant and can be used as below:</p>
<pre><code class="hljs css language-java"><span class="token class-name">StreamSource</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Event</span><span class="token punctuation">></span></span> source <span class="token operator">=</span> <span class="token class-name">PulsarSources</span><span class="token punctuation">.</span><span class="token function">pulsarReaderBuilder</span><span class="token punctuation">(</span>
                topicName<span class="token punctuation">,</span>
                <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token class-name">PulsarClient</span><span class="token punctuation">.</span><span class="token function">builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">serviceUrl</span><span class="token punctuation">(</span><span class="token string">"pulsar://localhost:6650"</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token class-name">Schema</span><span class="token punctuation">.</span><span class="token function">JSON</span><span class="token punctuation">(</span><span class="token class-name">Event</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token class-name">Message</span><span class="token operator">::</span><span class="token function">getValue</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>For a more detailed example, you can see the <a href="/docs/tutorials/pulsar">Apache Pulsar
Tutorial</a>.</p>
<h3><a class="anchor" aria-hidden="true" id="command-line-and-docker-improvements"></a><a href="#command-line-and-docker-improvements" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Command-line and Docker Improvements</h3>
<p>We've also made some improvements to the Jet command-line scripts. A
quick summary is below:</p>
<ul>
<li>You can now use the <code>config/jvm.options</code> file to control the JVM
options when starting Jet without having to set environment variables.</li>
<li>We've added support to export JMX metrics through Prometheus. It's
enough to specify a <code>PROMETHEUS_PORT</code> environment variable to start
exporting metrics using Prometheus.</li>
<li>Rebuild the docker image using a multi-stage process for a
smaller footprint.</li>
<li>You can use the <code>JET_MODULES</code> environment variable to auto-import
modules to Jet without having to copy files, which is especially
useful in a docker environment.</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="documentation-improvements"></a><a href="#documentation-improvements" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Documentation Improvements</h3>
<p>As part of the release, we've also made improvements and added new
sections to the documentation:</p>
<ul>
<li>Revamped and simplified <a href="/docs/operations/kubernetes">Jet on
Kubernetes</a> documentation.</li>
<li>Overhauled <a href="/docs/operations/docker">Running With Docker</a> with
additional information.</li>
<li>A new section on <a href="/docs/operations/gc-concerns">Garbage Collection</a> which
was the result of our <a href="/blog/2020/06/09/jdk-gc-benchmarks-part1">extensive research and benchmarking</a></li>
<li>Extended the documentation for <a href="/docs/api/stateless-transforms#mapusingpython">Python
transformations</a>.</li>
<li>Added additional docs about <a href="/docs/api/pipeline#adding-timestamps-to-a-stream">adding timestamps to a
stream</a>.</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="full-release-notes"></a><a href="#full-release-notes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Full Release Notes</h2>
<p>Members of the open source community that appear in these release notes:</p>
<ul>
<li>@caioguedes</li>
</ul>
<p>Thank you for your valuable contributions!</p>
<h3><a class="anchor" aria-hidden="true" id="new-features"></a><a href="#new-features" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>New Features</h3>
<ul>
<li>[core] [011] Add JSON file source as well as built-in functions for
parsing JSON strings (#2218, #2270)</li>
<li>[pipeline-api] [008] Add support for stage rebalancing (#2149)</li>
<li>[cdc] [005] New Change Data Capture Source for MySQL (#2142)</li>
<li>[cdc] [005] New Change Data Capture Source for PostgreSQL (#2247)</li>
<li>[cdc] [005] CDC Map Sink for keeping a Map in sync with a stream of
changes from the database (#2262)</li>
<li>[elasticsearch] [003] Added source and sink connectors for
Elasticsearch 5, 6, 7 (#2098, #2286, #2287)</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="enhancements"></a><a href="#enhancements" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Enhancements</h3>
<ul>
<li>[core] Support Hazelcast Serialization for ProcessorSupplier (#2298)</li>
<li>[core] Increase default parallelism for file and Avro source to 4
(#2359)</li>
<li>[pipeline-api] Support for keyFn and valueFn in map sink (#2198)</li>
<li>[jet-cli] Introduce --targets as the default command for specifying
where to connect and add it as a mixins for all comments (@caioguedes
#2276)</li>
<li>[jet-cli] Add support JET_MODULES environment variable to import
modules automatically without having to copy them (#2314)</li>
<li>[jet-cli] Support PROMETHEUS_PORT environment variable to start Jet
with prometheus metrics enabled (#2328)</li>
<li>[jet-cli] Add jvm.options file which can be used to specify JVM
options during startup (#2349)</li>
<li>[docker] Several Docker image improvements (hazelcast-jet-docker#27)</li>
<li>[grpc] Performance improvements to gRPC module (#2245)</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="fixes"></a><a href="#fixes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fixes</h3>
<ul>
<li>[core] Fix potential ClassCastException in onSnapshotPhase2Completed()
(#2338)</li>
<li>[core] Fix JobConfig.attachFile path resolution on Windows (#2357)</li>
<li>[jet-cli] Fix bad rolling filename causing misplaced files (#2270)</li>
<li>[jet-cli] Use exec in jet-start to support ctrl-C in docker
environment [#2307)</li>
<li>[avro] Add missing serializer for Avro Utf-8 class (#2358)</li>
</ul>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch</a></h1><p class="post-meta">June 23, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener">Marko Topolnik</a></p><div class="authorPhoto"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener"><img src="https://i.imgur.com/xuavzce.jpg" alt="Marko Topolnik"/></a></div></div></header><article class="post-content"><div><span><p>This post is a part of a series:</p>
<ul>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Part 1 (Intro and high-throughput streaming
benchmark)</a></li>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part 2 (batch workload benchmark)</a></li>
<li>Part 3 (you are here)</li>
<li><a href="/blog/2020/08/05/gc-tuning-for-jet">Part 4 (concurrent GC with green threads)</a></li>
</ul>
<p>This is a followup on Part 1 of the blog post series we started earlier
this month, analyzing the performance of modern JVMs on workloads that
are relevant to the use case of real-time stream processing.</p>
<p>As a quick recap, in Part 1 we tested the basic functionality of
<a href="https://github.com/hazelcast/hazelcast-jet">Hazelcast Jet</a> (sliding
window aggregation) on two types of workload: lightweight with a focus
on low latency, and heavyweight with a focus on the data pipeline
keeping up with high throughput and large aggregation state. For the
low-latency benchmarks we chose the JDK 14 as the most recent stable
version and three of its garbage collectors: Shenandoah, ZGC, and G1 GC.</p>
<p>Our finding that Shenandoah apparently fared worse than the other GCs
attracted some reactions, most notably from the Shenandoah team who
reproduced our finding, created an
<a href="https://bugs.openjdk.java.net/browse/JDK-8247358">issue</a>, came up with
a fix, and committed it to the jdk/jdk16 repository, all in the span of
a few days. The change pertains to the heuristics that decide how much
work the GC should do in the background in order to exactly match the
applications allocation rate. This component is called the <em>pacer</em>. It
was constantly detecting it's falling behind the application, triggering
a brief &quot;panic mode&quot; in order to catch up. The fix fine-tunes the
pacer's heuristics to make the background GC work more proactive.</p>
<p>Given this quick development, we wanted to test out the effects of the
fix, but also take the opportunity to zoom in on the low-latency
streaming case and make a more detailed analysis.</p>
<p>Here are our main conclusions:</p>
<ol>
<li>ZGC is still the winner and the only GC whose 99.99th percentile
latency stayed below 10 ms across almost all of our tested range</li>
<li>Shenandoah's pacer improvement showed a very strong effect, reducing
the latency by a factor of three, but still staying well above 10 ms
except in the very lowest part of our tested range</li>
<li>G1 kept its 99.99th percentile latency below 13 ms across a wide
range of throughputs</li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="the-jdk-we-tested"></a><a href="#the-jdk-we-tested" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The JDK We Tested</h2>
<p>Since this is all so fresh, we couldn't use an existing JDK release, not
even EA, to see the effects of the fix. JDK version 14.0.2 is slated to
be released on July 14. To nevertheless make progress, we took the
source code from the jdk14u tree, at the changeset number
<a href="http://hg.openjdk.java.net/jdk-updates/jdk14u/rev/e9d41bbaea38">57869:e9d41bbaea38</a>,
and applied the changeset number
<a href="https://hg.openjdk.java.net/jdk/jdk/rev/29b4bb22b5e2">59746:29b4bb22b5e2</a>
from the main jdk tree on top of it. The jdk14u tree is where JDK 14.0.2
will be released from and the changeset 59746:29b4bb22b5e2 applies the
patch resolving the mentioned Shenandoah issue.</p>
<h2><a class="anchor" aria-hidden="true" id="the-jvm-options"></a><a href="#the-jvm-options" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The JVM Options</h2>
<p>There are two HotSpot JVM options whose default values change
automatically when you use the ZGC so we had to decide which choice to
make when testing the other garbage collectors.</p>
<ul>
<li><p><code>-XX:-UseBiasedLocking</code>: biased locking has for a while been under
criticism that it causes higher latency spikes due to bias revocation
that must be done within a GC safepoint. In the upcoming JDK version
15, biased locking will be <a href="https://openjdk.java.net/jeps/374">disabled by default and
deprecated</a>. Any low-latency Java
application should have this disabled and we disabled it in all our
measurements.</p></li>
<li><p><code>-XX:+UseNUMA</code>: Shenandoah and ZGC can query the NUMA layout of the
host machine and optimize their memory layout accordingly. The only
reason why Shenandoah doesn't do it by default is a general precaution
against suddenly changing the behavior for upgrading users, but the
precaution is no longer necessary. It will be <a href="https://openjdk.java.net/jeps/163">enabled by
default</a> in upcoming JDK versions,
and we saw no harm in enabling it in all cases as well. <strong>Late
update</strong>: G1 can also optimize for the NUMA layout, but we didn't use
<code>UseNUMA</code> for it. However, we also checked the c5.4xlarge instance
with <code>numactl</code> and it indicated that the entire machine was a single
NUMA node anyway.</p></li>
</ul>
<p>There is also a JVM feature that is simply incompatible with ZGC's
colored pointers: compressed object pointers. In other words, ZGC
applies <code>-XX:-UseCompressedOops</code> without the option to enable it.
A compressed pointer is just 32 bits long but handles heaps of up to
32 GB and it's usually beneficial to both memory usage and performance.
We left this option enabled for Shenandoah.</p>
<p>For the G1 collector, we also set <code>-XX:MaxGCPauseMillis=5</code>, same as in
the previous testing round, because the default of 200 milliseconds is
optimized for throughput and the G1 can give you much better latency
than that.</p>
<p>We performed all our tests on an EC2 c5.4xlarge instance. It has 16
vCPUs and 32 GB of RAM.</p>
<h2><a class="anchor" aria-hidden="true" id="the-data-pipeline"></a><a href="#the-data-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Data Pipeline</h2>
<p>To get a more nuanced insight into the performance, we made some
improvements to the testing code. Whereas in the first iteration we just
reported the maximum latency, this time around we wanted to capture the
entire latency profile. To this end we had to increase the number of
reports per second the pipeline outputs. Initially we set it to 10 times
per second, a number which results in too few data points for the
latency chart. The pipeline in this round emits 100 reports per second.
The event rate and the length of the time window are the same: 1 million
events per second and 10 seconds, respectively. This results in 1,000
hashtables each holding 10,000 keys as the aggregation state. We tested
across a wide range of keyset sizes, starting from 5,000 up to 105,000.</p>
<p>Note that the size of the keyset, somewhat counterintuitively, does not
affect the size of the aggregation state. As long as the 10,000 input
events received during one time slice of 10 milliseconds all use
distinct keys, the state is fixed as described above. Only in the lowest
setting, 5,000, the state is half as large since every hashtable
contains just 5,000 keys.</p>
<p>What the keyset size does affect is allocation rate. The pipeline emits
the full keyset every 10 milliseconds. For example, with 50,000 keys
that's 5,000,000 result items per second. If we add to that the rate of
the input stream (a fixed million events per second), we get a value
that is a good proxy for the overall allocation rate. This is why we
chose combined input+output rate as the x-axis value in the charts that
we'll be showing below.</p>
<p>Here is the basic code of the pipeline, available on
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/round-2/src/main/java/org/example/StreamingRound2.java">GitHub</a>:</p>
<pre><code class="hljs css language-java"><span class="token class-name">StreamStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">></span></span> source <span class="token operator">=</span> p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">longSource</span><span class="token punctuation">(</span>EVENTS_PER_SECOND<span class="token punctuation">)</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">withNativeTimestamps</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">StreamStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Tuple2</span><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">,</span> <span class="token class-name">Long</span><span class="token punctuation">></span><span class="token punctuation">></span></span> latencies <span class="token operator">=</span> source
        <span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> NUM_KEYS<span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span><span class="token function">sliding</span><span class="token punctuation">(</span>WIN_SIZE_MILLIS<span class="token punctuation">,</span> SLIDING_STEP_MILLIS<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>kwr <span class="token operator">-></span> kwr<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> DIAGNOSTIC_KEYSET_DOWNSAMPLING_FACTOR <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">mapStateful</span><span class="token punctuation">(</span><span class="token class-name">DetermineLatency</span><span class="token operator">::</span><span class="token keyword">new</span><span class="token punctuation">,</span> <span class="token class-name">DetermineLatency</span><span class="token operator">::</span><span class="token function">map</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

latencies<span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>t2 <span class="token operator">-></span> t2<span class="token punctuation">.</span><span class="token function">f0</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> TOTAL_TIME_MILLIS<span class="token punctuation">)</span>
         <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>t2 <span class="token operator">-></span> <span class="token class-name">String</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token string">"%d,%d"</span><span class="token punctuation">,</span> t2<span class="token punctuation">.</span><span class="token function">f0</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> t2<span class="token punctuation">.</span><span class="token function">f1</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
         <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">files</span><span class="token punctuation">(</span><span class="token string">"/home/ec2-user/laten"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
latencies
      <span class="token punctuation">.</span><span class="token function">mapStateful</span><span class="token punctuation">(</span><span class="token class-name">RecordLatencyHistogram</span><span class="token operator">::</span><span class="token keyword">new</span><span class="token punctuation">,</span> <span class="token class-name">RecordLatencyHistogram</span><span class="token operator">::</span><span class="token function">map</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">files</span><span class="token punctuation">(</span><span class="token string">"/home/ec2-user/bench"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>The main part, sliding window aggregation, remains the same, but the
following stages that process the results are new. We write the data to
two files: <code>laten</code>, containing all the raw latency data points, and
<code>bench</code>, containing an <a href="https://hdrhistogram.github.io/HdrHistogram/plotFiles.html">HDR
Histogram</a>
of the latencies.</p>
<p>Another key difference is that, in the original post, we measured the
latency of <em>completing</em> to emit a result set, but here we measure the
latency of <em>starting</em> to emit it. Since we are changing the size of the
output, if we kept measuring the completion latency, we'd be introducing
a different amount of application-induced latency at each data point.</p>
<p>There's another, relatively minor technical point worth mentioning:
since we tested on a cloud server instance, we used Jet's client-server
mode, which means we separately start a Jet node and then deploy the
pipeline to it using Jet's command <code>jet submit</code>. The code available on
GitHub is the client code and the Jet server code was a build from the
Jet master branch before Jet 4.2 was released. We expect all the results
to be reproducible with the <a href="https://github.com/hazelcast/hazelcast-jet/releases/download/v4.2/hazelcast-jet-4.2.tar.gz">Jet 4.2
release</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="what-exactly-we-measured"></a><a href="#what-exactly-we-measured" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What Exactly We Measured</h2>
<p>We measured the latency as the timestamp at which the pipeline emits a
given result minus the timestamp to which the result pertains, giving us
end-to-end latency (the only kind the user actually cares about).</p>
<p>Keep especially in mind that latency does not equal a GC pause.
Normally, neither Shenandoah nor ZGC enter anything more than a
millisecond of GC pause, but their background work shares the limited
system capacity with the application. With G1 the equivalence is much
stronger and its 10-20 millisecond latencies are primarily the result of
GC pauses that long.</p>
<h2><a class="anchor" aria-hidden="true" id="the-measurements"></a><a href="#the-measurements" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Measurements</h2>
<p>To come up with the charts below, for each data point we let the
pipeline warm up for 20 seconds and then gathered the latencies for 4
minutes, collecting 24,000 samples.</p>
<p>Here is the latency histogram taken at 2 million items per second,
close to the bottom of our range:</p>
<p><img src="/blog/assets/2020-06-23-histo-2m.png" alt="Latency on JDK 14.0.2 pre-release, 2M items per second"></p>
<p>Unpatched Shenandoah seems like the winner, except for the single
worst-case latency. With the patch applied, latency increases sooner but
more gently and doesn't have a strong peak. ZGC comes somewhere between,
but overall all three cases show pretty similar behavior. G1 is clearly
worse and its latency exceeds the 10 ms mark before even reaching the
99th percentile. Since our pipeline emits a new result set ever 10 ms,
we shall consider 10 ms as the cutoff point: everything above 10 ms
should be considered a failure for our use case.</p>
<p>Next, let's take a look at the latencies after increasing the throughput
a bit, to 3 million items per second:</p>
<p><img src="/blog/assets/2020-06-23-histo-3m.png" alt="Latency on JDK 14.0.2 pre-release, 3M items per second"></p>
<p>Wow, what an unexpected difference! Now we can clearly see the pacer
improvement doing its thing, lowering the latency about threefold.
However, even with the improvement, Shenandoah unfortunately crosses the
10 ms mark pretty early, below the 99th percentile, and is worse than G1
at almost every percentile. ZGC and G1 score basically the same as
before.</p>
<p>Note also the very regular shape of the pale blue curve (unpatched
Shenandoah): this is a symptom of the way a single bad event trickles
down into the lower latency percentiles. For example, if one result is
late by 50 ms, that means it has already caused the next four results to
have at least the latencies of 40, 30, 20, and 10 ms, even if they would
be emitted instantaneously.</p>
<p>Next, let's zoom out to an overview of the entire range of throughputs
we benchmarked, taking the 99.99%ile as the reference point and showing
its dependence on throughput. To paint an intuitive picture, 99.99%
latency tells you that, in any span of 100 seconds you look at, you're
likely to find a latency spike at least that large. Here's the chart:</p>
<p><img src="/blog/assets/2020-06-23-latencies-jdk14.png" alt="Latencies on JDK 14.0.2 pre-release"></p>
<p>Here are some things to note:</p>
<ol>
<li>ZGC stays below 10 ms over a large part of the range, up to 8 M items
per second. This makes it not just the winner, but the only choice
for the range from 2 million to 8 million items per second.</li>
<li>The G1 collector is unphased by the differences in throughput. While
its latency is never under 10 milliseconds, it keeps its level over
the entire tested range and more. Its latency even improves a bit
with higher loads.</li>
<li>At 9.5 M items per second, ZGC shows a remarkable recovery.
Sandwiched between the latencies of 92 and 209 milliseconds, at this
exact throughput it achieves 10 ms latency! We of course thought it
was a measurement error and repeated it for three times, but the
result was consistent. Maybe there's a lesson in there for the ZGC
engineers.</li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="a-sneak-peek-into-upcoming-versions"></a><a href="#a-sneak-peek-into-upcoming-versions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>A Sneak Peek into Upcoming Versions</h2>
<p>As a preview into what's coming up in OpenJDK, we also took a look at
the <a href="https://download.java.net/java/early_access/jdk15/28/GPL/openjdk-15-ea+28_linux-x64_bin.tar.gz">Early Access release 27 of JDK
15</a>.
Shenandoah's pacer improvement is not applied in it, so to properly test
Shenandoah's prospects we used a build available at
<a href="https://builds.shipilev.net/openjdk-jdk/">builds.shipilev.net/openjdk-jdk</a>,
specifically one that reports its version as <code>build 16-testing+0-builds.shipilev.net-openjdk-jdk-b1282-20200611</code>. Out of
interest we also doubled our throughput range to capture more of the
behavior after the latency exceeds 10 milliseconds. Here's what we got:</p>
<p><img src="/blog/assets/2020-06-23-latencies-latest.png" alt="Latencies on upcoming JDK versions"></p>
<p>We can see a nice incremental improvement for the ZGC: less than 5 ms
latencies at throughputs below 5 M/s. Shenandoah's curve is even a bit
worse at 2.5-3 M per second, but generally pretty similar. At higher
loads we can see ZGC's failure mode is quite a bit more severe than
Shenandoah's, although just how bad the latency gets doesn't affect the
bottom line of a scenario where everything above 10 ms is already a
failure.</p>
<p>The wider chart also gives better insight into the stability of G1,
keeping itself below 20 ms all the way up to 20 M items per second.</p>
<p><em>If you enjoyed reading this post, check out Jet at
<a href="https://github.com/hazelcast/hazelcast-jet">GitHub</a> and give us a
star!</em></p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Performance of Modern Java on Data-Heavy Workloads: Batch Processing</a></h1><p class="post-meta">June 9, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener">Marko Topolnik</a></p><div class="authorPhoto"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener"><img src="https://i.imgur.com/xuavzce.jpg" alt="Marko Topolnik"/></a></div></div></header><article class="post-content"><div><span><p>This post is a part of a series:</p>
<ul>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Part 1 (Intro and high-throughput streaming
benchmark)</a></li>
<li>Part 2 (you are here)</li>
<li><a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Part 3 (low-latency benchmark)</a></li>
<li><a href="/blog/2020/08/05/gc-tuning-for-jet">Part 4 (concurrent GC with green threads)</a></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="batch-pipeline-benchmark"></a><a href="#batch-pipeline-benchmark" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Batch Pipeline Benchmark</h2>
<p>A batch pipeline processes a finite amount of stored data. There are no
running results, we need the output of the aggregate function applied to
the entire dataset. This changes our performance requirements: the key
factor in streaming, latency, doesn't exist here since we are not
processing data in real time. The only metric that matters is the total
run time of the pipeline.</p>
<p>For this reason we considered the Parallel GC as a relevant candidate.
In the first testing round, on a single node, it actually delivered the
best throughput (but only after GC tuning). However, it achieves that
throughput at the expense of GC pause duration. In a cluster, whenever
any node enters a GC pause, it stalls the whole data pipeline. Since
individual nodes enter GC pauses at different times, the amount of time
spent in GC goes up with every node you add to the cluster. We explored
this effect by comparing single-node tests with tests in a three-node
cluster.</p>
<p>On the flip side, we did not consider the experimental low-latency
collectors in this round since their very short GC pauses have no effect
on the test result, and they achieve them at the expense of throughput.</p>
<h3><a class="anchor" aria-hidden="true" id="single-node-benchmark-the-pipeline"></a><a href="#single-node-benchmark-the-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Single-Node Benchmark: The Pipeline</h3>
<p>For the single-node batch benchmark we used this simple pipeline, full
code on
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/master/src/main/java/org/example/BatchBenchmark.java">GitHub</a>:</p>
<pre><code class="hljs css language-java">p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span>longSource<span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">// Introduced in Jet 4.2</span>
 <span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> NUM_KEYS<span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">summingLong</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>e <span class="token operator">-></span> <span class="token punctuation">(</span>e<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token number">0</span>xFF_FFFFL<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<p>The source is again a self-contained mock source that just emits a
sequence of <code>long</code> numbers and the key function is defined so that the
grouping key cycles through the key space: 0, 1, 2, ..., <code>NUM_KEYS</code>, 0,
1, 2, ... This means that, over the first cycle, the pipeline observes
all the keys and builds up a fixed data structure to hold the
aggregation results. Over the following cycles it just updates the
existing data. This aligns perfectly with the Generational Garbage
Hypothesis: the objects either last through the entire computation or
are short-lived temporary objects that become garbage very soon after
creation.</p>
<p>We let our source emit 400 million items and had 100 million distinct
keys, so we cycled four times over the same keys.</p>
<p>The <code>.rebalance()</code> operator changes Jet's default <a href="/docs/concepts/dag#group-and-aggregate-transform-needs-data-partitioning">two-stage
aggregation</a>
to single-stage. It exhibited more predictable behavior in our
benchmark.</p>
<p>We also tested a variant where the aggregate operation uses a boxed
<code>Long</code> instance as state, producing garbage every time the running score
is updated. In this case many objects die after having spent substantial
time in the old generation. For this variant we had to reduce the number
of keys to 70 million, with 100 million the GC pressure was too high.</p>
<p>For the batch pipeline we didn't focus on the low-latency collectors
since they have nothing to offer in this case. Also, because we saw
earlier that JDK 14 performs much the same as JDK 11, we just ran one
test to confirm it, but otherwise focused on JDK 8 vs. JDK 11 and
compared the JDK 8 default Parallel collector with G1.</p>
<h3><a class="anchor" aria-hidden="true" id="single-node-benchmark-the-results"></a><a href="#single-node-benchmark-the-results" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Single-Node Benchmark: The Results</h3>
<p>For single-node testing, we ran the benchmark on a laptop with 16 GB RAM
and a 6-core Intel Core i7. We used a heap size of 10 GB.</p>
<p>Initially we got very bad performance out of the Parallel collector and
had to resort to GC tuning. For this purpose we highly recommend using
VisualVM and its Visual GC plugin. When you set the frame rate to the
highest setting (10 FPS), you can enjoy a very fine-grained visual
insight into how the interplay between your application's allocation and
the GC works out. By watching these live animations for a while, we
realized that the main issue was a too large slice of RAM given to the
new generation. By default the ratio between Old and New generations is
just 2:1, and it is not dynamically adaptable at runtime. Based on this
we decided to try with <code>-XX:NewRatio=8</code> and it completely changed the
picture. Now Parallel was turning in the best times overall. We also
used <code>-XX:MaxTenuringThreshold=2</code> to reduce the copying of data between
the Survivor spaces, since in the pipeline the temporary objects die
pretty soon.</p>
<p>Now, on to the results. The only relevant metric in this batch pipeline
benchmark is the time for the job to complete. To visualize the results
we took the reciprocal of that, so the charts show throughput in items
per second. Here are our results on a single node:</p>
<p><img src="/blog/assets/2020-06-01-batch-mutable.png" alt="Single-node Batch pipeline with garbage-free aggregation"></p>
<p><img src="/blog/assets/2020-06-01-batch-boxed.png" alt="Single-node Batch pipeline with garbage-producing aggregation"></p>
<p>Comparing the two charts we can see that garbage-free aggregation gave a
throughput boost of around 30-35%, and that's despite the larger keyset
we used for it. G1 on JDK 8 was the worst performer and the fine-tuned
Parallel on JDK 11 was the best. G1 on JDK 11 wasn't far behind. Note
that we didn't have to touch anything in the configuration of G1, which
is an important fact. GC tuning is highly case-specific, the results may
dramatically change with e.g., more data, and it must be applied to the
entire cluster, making it specifically tuned for one kind of workload.</p>
<p>Here's the performance of the default Parallel GC compared to the tuned
version we used for testing:</p>
<p><img src="/blog/assets/2020-06-01-batch-parallel.png" alt="Throughput of the Parallel Collector with and without tuning"></p>
<p>With 10 GB of heap it failed completely, stuck in back-to-back Full GC
operations each taking about 7 seconds. With more heap it managed to
make some progress, but was still hampered with very frequent Full GCs.
Note that we got the above results for the most favorable case, with
garbage-free aggregation.</p>
<h3><a class="anchor" aria-hidden="true" id="three-node-cluster-benchmark-the-pipeline"></a><a href="#three-node-cluster-benchmark-the-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Three-Node Cluster Benchmark: The Pipeline</h3>
<p>To properly benchmark in the cluster, we had to use a bit more complex
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/master/src/main/java/org/example/ClusterBatchBenchmark.java">pipeline</a>:</p>
<pre><code class="hljs css language-java">p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">longSource</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>n <span class="token operator">-></span> <span class="token punctuation">{</span>
     <span class="token class-name">Long</span><span class="token punctuation">[</span><span class="token punctuation">]</span> items <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Long</span><span class="token punctuation">[</span>SOURCE_STEP<span class="token punctuation">]</span><span class="token punctuation">;</span>
     <span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">setAll</span><span class="token punctuation">(</span>items<span class="token punctuation">,</span> i <span class="token operator">-></span> n <span class="token operator">+</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span>
     <span class="token keyword">return</span> <span class="token function">traverseArray</span><span class="token punctuation">(</span>items<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> NUM_KEYS<span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token class-name">AggregateOperations</span><span class="token punctuation">.</span><span class="token function">summingLong</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>e <span class="token operator">-></span> e<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">1_000_000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">;</span>
</code></pre>
<p>Since the source is non-parallel, we applied some optimizations so it
doesn't become a bottleneck. We let the source emit the numbers 0, 10,
20, ... and then applied a parallelized <code>flatMap</code> stage that
interpolates the missing numbers. We also used <code>rebalance()</code> between the
source and <code>flatMap</code>, spreading the data across the cluster. We applied
rebalancing again before entering the main stage, keyed aggregation.
After the aggregation stage we first reduce the output to every
millionth key-value pair and then send it to the logger. We used one
billion data items and a keyset of half a billion.</p>
<p>Same as on single-node, we tested both this pipeline, with a garbage-free
aggregation, and a modified one with garbage-producing aggregation.</p>
<h3><a class="anchor" aria-hidden="true" id="three-node-cluster-benchmark-the-results"></a><a href="#three-node-cluster-benchmark-the-results" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Three-Node Cluster Benchmark: The Results</h3>
<p>We performed this benchmark on an AWS cluster of three c5d.4xlarge
instances. They have 16 virtualized CPU cores and 32 GB of RAM. The
network is 10 Gbit/s. Here are the results:</p>
<p><img src="/blog/assets/2020-06-01-batch-cluster-mutable.png" alt="3-Node Batch pipeline with garbage-free aggregation"></p>
<p><img src="/blog/assets/2020-06-01-batch-cluster-boxed.png" alt="3-Node Batch pipeline with garbage-producing aggregation"></p>
<p>In passing, let's note the overall increase in throughput compared to
single-node benchmarks, about three times. That's the advantage of
distributed processing. As for collectors, G1 on JDK 11 is the clear
winner in both tests. Another striking result is the almost nonexistent
bar for G1 on JDK 8, however there's a deeper story here that affects
other measurements as well, for example the apparent advantage of
Parallel GC on JDK 8 vs. JDK 11. It has to do with the effect we noted
at the outset: a GC pause on any one member halts the processing on the
entire cluster. G1 on JDK 8 enters very long GC pauses, more than a
minute. This is enough to trigger the cluster's failure detector and
consider the node dead. The job fails, the cluster reshapes itself, and
then the job restarts on just two nodes. This, naturally, fails even
sooner because there's more data on each member. In the meantime the
kicked-out node has rejoined, so the job restarts on two nodes again,
but different ones. We end up in an endless loop of job restarts.</p>
<p>The Parallel collector's GC pauses stopped short of bringing down the
cluster, but it fared significantly worse than in single-node tests.
Here it was 30% behind the G1 on JDK 11. With a bigger cluster this
would get even worse.</p>
<p>Compared to all other tests, it is surprising to see Parallel on JDK 8
win over JDK 11, however this is due to a very lucky coincidence that,
in those particular test runs, the Full GC pauses got synchronized on
all nodes, parallelizing the effort of the GC. Clearly, this is not a
reliable effect.</p>
<p>Even though in the particular benchmark setup which we report here, we
didn't observe the catastrophic consequence of long GC pauses on cluster
stability while using the Parallel collector, it is more of a chance
outcome. In other tests, where we used a larger heap and more data, or
the same heap but with less headroom left, the Parallel collector did
cause the same damage. However, even when it doesn't cause outright
failure, the charts show the advantage it had on a single node has
disappeared. You can expect the results to get worse with each further
node you add to the cluster.</p>
<p>The JDK 11 G1 collector, on the other hand, was producing GC pauses of a
sufficiently short duration that the rest of the pipeline didn't get
stalled. The pipeline has mechanisms that dampen out short hiccups and
as long as the GC pauses are within acceptable limits (up to some 150
ms), the effect of GC stays local.</p>
<p><em>If you enjoyed reading this post, check out Jet at
<a href="https://github.com/hazelcast/hazelcast-jet">GitHub</a> and give us a
star!</em></p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming</a></h1><p class="post-meta">June 9, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener">Marko Topolnik</a></p><div class="authorPhoto"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener"><img src="https://i.imgur.com/xuavzce.jpg" alt="Marko Topolnik"/></a></div></div></header><article class="post-content"><div><span><p>This post is a part of a series:</p>
<ul>
<li>Part 1 (you are here)</li>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part 2 (batch workload benchmark)</a></li>
<li><a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Part 3 (low-latency benchmark)</a></li>
<li><a href="/blog/2020/08/05/gc-tuning-for-jet">Part 4 (concurrent GC with green threads)</a></li>
</ul>
<p>The Java runtime has been evolving more rapidly in recent years and,
after 15 years, we finally got a new default garbage collector: the
G1. Two more GCs are on their way to production and are available as
experimental features: Oracle's ZGC and OpenJDK's Shenandoah. We at
Hazelcast thought it was time to put all these new options to the test
and find which choices work well with workloads typical for our
distributed stream processing engine, <a href="https://github.com/hazelcast/hazelcast-jet">Hazelcast Jet</a>.</p>
<p>Jet is being used for a broad spectrum of use cases, with different
latency and throughput requirements. Here are three important
categories:</p>
<ol>
<li>Low-latency unbounded stream processing, with moderate state.
Example: detecting trends in 100 Hz sensor data from 10,000 devices
and sending corrective feedback within 10-20 milliseconds.</li>
<li>High-throughput, large-state unbounded stream processing. Example:
tracking GPS locations of millions of users, inferring their velocity
vectors.</li>
<li>Old-school batch processing of big data volumes. The relevant measure
is time to complete, which implies a high throughput demand. Example:
analyzing a day's worth of stock trading data to update the risk
exposure of a given portfolio.</li>
</ol>
<p>At the outset, we can observe the following:</p>
<ul>
<li>in scenario 1 the latency requirements enter the danger zone of GC
pauses: 100 milliseconds, something traditionally considered an
excellent result for a worst-case GC pause, may be a showstopper for
many use cases</li>
<li>scenarios 2 and 3 are similar in terms of demands on the garbage
collector. Less strict latency, but large pressure on the tenured
generation</li>
<li>scenario 2 is tougher because latency, even if less so than in
scenario 1, is still relevant</li>
</ul>
<p>We tried the following combinations:</p>
<ol>
<li>JDK 8 with the default Parallel collector and the optional
ConcurrentMarkSweep and G1</li>
<li>JDK 11 with the default G1 collector and the optional Parallel</li>
<li>JDK 14 with the default G1 as well as the experimental ZGC and
Shenandoah</li>
</ol>
<p>And here are our overall conclusions:</p>
<ol>
<li>On modern JDK versions, the G1 is one monster of a collector. It
handles heaps of dozens of GB with ease (we tried 60 GB), keeping
maximum GC pauses within 200 ms. Under extreme pressure it doesn't
show brittleness with catastrophic failure modes. Instead the Full GC
pauses rise into the low seconds range. Its Achilles' heel is the
upper bound on the GC pause in favorable low-pressure conditions,
which we couldn't push lower than 20-25 ms.</li>
<li>JDK 8 is an antiquated runtime. The default Parallel collector enters
huge Full GC pauses and the G1, although having less frequent Full
GCs, is stuck in an old version that uses just one thread to perform
it, resulting in even longer pauses. Even on a moderate heap of 12
GB, the pauses were exceeding 20 seconds for Parallel and a full
minute for G1. The ConcurrentMarkSweep collector is strictly worse
than G1 in all scenarios, and its failure mode are multi-minute Full
GC pauses.</li>
<li>The ZGC, while allowing substantially less throughput than G1, was
very good in that one weak area of G1, occasionally increasing our
latency by up to 10 ms under light load.</li>
<li>Shenandoah was a disappointment with occasional, but nevertheless
regular, latency spikes up to 220 ms in the low-pressure regime.</li>
<li>Neither ZGC nor Shenandoah showed as smooth failure modes as G1. They
exhibited brittleness, with the low-latency regime suddenly giving
way to very long pauses and even OOMEs.</li>
</ol>
<p>This post is Part 1 of a two-part series and presents our findings for
the two streaming scenarios. In <a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part
2</a> we'll present the results
for batch processing.</p>
<h2><a class="anchor" aria-hidden="true" id="streaming-pipeline-benchmark"></a><a href="#streaming-pipeline-benchmark" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Streaming Pipeline Benchmark</h2>
<p>For the streaming benchmarks, we used the code available
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/master/src/main/java/org/example/StreamingBenchmark.java">here</a>,
with some minor variations between the tests. Here is the main part, the
Jet pipeline:</p>
<pre><code class="hljs css language-java"><span class="token class-name">StreamStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">></span></span> source <span class="token operator">=</span> p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">longSource</span><span class="token punctuation">(</span>ITEMS_PER_SECOND<span class="token punctuation">)</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">withNativeTimestamps</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// Introduced in Jet 4.2</span>
source<span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> NUM_KEYS<span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span><span class="token function">sliding</span><span class="token punctuation">(</span>SECONDS<span class="token punctuation">.</span><span class="token function">toMillis</span><span class="token punctuation">(</span>WIN_SIZE_SECONDS<span class="token punctuation">)</span><span class="token punctuation">,</span> SLIDING_STEP_MILLIS<span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>kwr <span class="token operator">-></span> kwr<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> DIAGNOSTIC_KEYSET_DOWNSAMPLING_FACTOR <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span><span class="token function">tumbling</span><span class="token punctuation">(</span>SLIDING_STEP_MILLIS<span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span>wr <span class="token operator">-></span> <span class="token class-name">String</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token string">"time %,d: latency %,d ms, cca. %,d keys"</span><span class="token punctuation">,</span>
              <span class="token function">simpleTime</span><span class="token punctuation">(</span>wr<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              NANOSECONDS<span class="token punctuation">.</span><span class="token function">toMillis</span><span class="token punctuation">(</span><span class="token class-name">System</span><span class="token punctuation">.</span><span class="token function">nanoTime</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">-</span> wr<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
              wr<span class="token punctuation">.</span><span class="token function">result</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> DIAGNOSTIC_KEYSET_DOWNSAMPLING_FACTOR<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>This pipeline represents use cases with an unbounded event stream where
the engine is asked to perform sliding window aggregation. You need this
kind of aggregation, for example, to obtain the time derivative of a
changing quantity, remove high-frequency noise from the data (smoothing)
or measure the intensity of the occurrence of some event (events per
second). The engine can first split the stream by some category (for
example, each distinct IoT device or smartphone) into substreams and
then independently track the aggregated value in each of them. In
Hazelcast Jet the sliding window moves in fixed-size steps that you
configure. For example, with a sliding step of 1 second you get a
complete set of results every second, and if the window size is 1
minute, the results reflect the events that occurred within the last
minute.</p>
<p>Some notes:</p>
<p>The code is entirely self-contained with no outside data sources or
sinks. We use a mock data source that simulates an event stream with
exactly the chosen number of events per second. Consecutive event
timestamps are an equal amount of time apart. The source never emits an
event whose timestamp is still in the future, but otherwise emits them
as fast as possible.</p>
<p>If the pipeline falls behind, events will be &quot;buffered&quot; but without any
storage. After falling behind, the pipeline must catch up by ingesting
data as fast as it can. Since our source is non-parallel, the limit on
its throughput was about 2.2 million events per second. We used 1
million simulated events per second, leaving a catching-up headroom of
1.2 million per second.</p>
<p>The pipeline measures its own latency by comparing the timestamp of an
emitted sliding window result with the actual wall-clock time. In more
detail, there are two aggregation stages with filtering between them. A
single sliding window result consists of many items, each for one
substream, and we're interested in the latency of the last-emitted
item. For this reason we first filter out most of the output, keeping
every 10,000th entry, and then direct the thinned-out stream to the
second, non-keyed tumbling window stage that notes the result size and
measures the latency. Non-keyed aggregation is not parallelized, so we
get a single point of measurement. The filtering stage is parallel and
data-local so the impact of the additional aggregation step is very
small (well below 1 ms).</p>
<p>We used a trivial aggregate function: counting, in effect obtaining the
events/second metric of the stream. It has minimal state (a single
<code>long</code> number) and produces no garbage. For any given heap usage in
gigabytes, such a small state per key implies the worst case for the
garbage collector: a very large number of objects. GC overheads scale
much more with object count than heap size. We also tested a variant
that computes the same aggregate function, but with a different
implementation that produces garbage.</p>
<p>We performed most of the streaming benchmarks on a single node since our
focus was the effect of memory management on pipeline performance and
network latency just adds noise into the picture. We repeated some key
tests on a three-node Amazon EC2 cluster to validate our prediction that
cluster performance won't affect our conclusions. You can find a more
detailed justification for this towards the end of <a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part
2</a>.</p>
<p>We excluded the Parallel collector from the results for streaming
workloads because the latency spikes it introduces would be unacceptable
in pretty much any real-life scenario.</p>
<h3><a class="anchor" aria-hidden="true" id="scenario-1-low-latency-moderate-state"></a><a href="#scenario-1-low-latency-moderate-state" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scenario 1: Low Latency, Moderate State</h3>
<p>For the first scenario we used these parameters:</p>
<ul>
<li>OpenJDK 14</li>
<li>JVM heap size 4 gigabytes</li>
<li>for G1, -XX:MaxGCPauseMillis=5</li>
<li>1 million events per second</li>
<li>50,000 distinct keys</li>
<li>30-second window sliding by 0.1 second</li>
</ul>
<p>In this scenario there's less than 1 GB heap usage. The collector is not
under high pressure, it has enough time to perform concurrent GC in the
background. These are the maximum pipeline latencies we observed with
the three garbage collectors we tested:</p>
<p><img src="/blog/assets/2020-06-01-light-streaming-latency.png" alt="Pipeline Latency with Light Streaming"></p>
<p>Note that these numbers include a fixed time of about 3 milliseconds to
emit the window results. The chart is pretty self-explanatory: the
default collector, G1, is pretty good on its own, but if you need even
better latency, you can use the experimental ZGC collector. We couldn't
reduce the latency spikes below 10 milliseconds, however we did note
that, in the case of ZGC and Shenandoah, they weren't due to outright GC
pauses but rather short periods of increased background GC work.
Shenandoah's overheads occasionally raised latency above 200 ms.</p>
<h3><a class="anchor" aria-hidden="true" id="scenario-2-large-state-less-strict-latency"></a><a href="#scenario-2-large-state-less-strict-latency" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Scenario 2: Large State, Less Strict Latency</h3>
<p>In scenario 2 we assume that, for various reasons outside our control,
(e.g., mobile network), the latency can grow into low seconds, which
relaxes the requirements we must impose on our stream processing
pipeline. On the other hand, we may be dealing with much larger data
volumes, on the order of millions or dozens of millions of keys.</p>
<p>In this scenario we can provision our hardware so it's heavily utilized,
relying on the GC to manage a large heap instead of spreading out the
data over many cluster nodes.</p>
<p>We performed many tests with different combinations to find out how the
interplay between various factors causes the runtime to either keep up
or fall behind. In the end we found two parameters that determine this:</p>
<ol>
<li>number of entries stored in the aggregation state</li>
<li>demand on the catching-up throughput</li>
</ol>
<p>The first one corresponds to the number of objects in the tenured
generation. Sliding window aggregation retains objects for a significant
time (the length of the window) and then releases them. This goes
directly against the Generational Garbage Hypothesis, which states that
objects will either die young or live forever. This regime puts the
strongest pressure on the GC, and since the GC effort scales with the
number of live objects, performance is highly sensitive to this
parameter.</p>
<p>The second parameter relates to how much GC overhead the application can
tolerate. To explain it better, let's use some diagrams. A pipeline
performing windowed aggregation goes through three distinct steps:</p>
<ol>
<li>processing events in real time, as they arrive</li>
<li>emitting the sliding window results</li>
<li>catching up with the events received while in step 2</li>
</ol>
<p>The three phases can be visualized as follows:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-1.png" alt="Phases of the sliding window computation"></p>
<p>If emitting the window result takes longer, we get a situation like this:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-2.png" alt="Phases of the sliding window computation"></p>
<p>Now the headroom has shrunk to almost nothing, the pipeline is barely
keeping up, and any temporary hiccups like an occasional GC pause will
cause latency to grow and recover at a very slow pace.</p>
<p>If we change this picture and present just the average event ingestion
rate after window emission, we get this:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-3.png" alt="Phases of the sliding window computation"></p>
<p>We call the height of the yellow rectangle &quot;catchup demand&quot;: it is the
demand on the throughput of the source. If it exceeds the actual maximum
throughput, the pipeline fails.</p>
<p>This is how it would look if window emission took way too long:</p>
<p><img src="/blog/assets/2020-06-01-sliding-window-4.png" alt="Phases of the sliding window computation"></p>
<p>The area of the red and the yellow rectangles is fixed, it corresponds
to the amount of data that must flow through the pipeline. Basically,
the red rectangle &quot;squeezes out&quot; the yellow one. But the yellow
rectangle's height is actually limited, in our case to 2.2 million
events per second. So whenever it would be taller than the limit, we'd
have a failing pipeline whose latency grows without bounds.</p>
<p>We worked out the formulas that predict the sizes of the rectangles for
a given combination of event rate, window size, sliding step and keyset
size, so that we could determine the catchup demand for each case.</p>
<p>Now that we have two more-or-less independent parameters derived from
many more parameters describing each individual setup, we can create a
2D-chart where each benchmark run has a point on it. We assigned a color
to each point, telling us whether the given combination worked or
failed. For example, for JDK 14 with G1 on a developer's laptop, we got
this picture:</p>
<p><img src="/blog/assets/2020-06-01-viable-combinations-jdk14.png" alt="Viable combinations of catchup demand and storage, JDK 14 and G1"></p>
<p>We made the distinction between &quot;yes&quot;, &quot;no&quot; and &quot;gc&quot;, meaning the
pipeline keeps up, doesn't keep up due to lack of throughput, or doesn't
keep up due to frequent long GC pauses. Note that the lack of throughput
can also be caused by concurrent GC activity and frequent short GC
pauses. In the end, the distinction doesn't matter a lot.</p>
<p>You can make out a contour that separates the lower-left area where
things work out from the rest of the space, where they fail. We made
the same kind of chart for other combinations of JDK and GC, extracted
the contours, and came up with this summary chart:</p>
<p><img src="/blog/assets/2020-06-01-viable-combinations.png" alt="Viable combinations of catchup demand and storage, JDK 14 and G1"></p>
<p>For reference, the hardware we used is a MacBook Pro 2018 with a 6-core
Intel Core i7 and 16 GB DDR4 RAM, configuring <code>-Xmx10g</code> for the JVM.
However, we do expect the overall relationship among the combinations to
remain the same on a broad range of hardware parameters. The chart
visualizes the superiority of the G1 over others, the weakness of the G1
on JDK 8, and the weakness of the experimental low-latency collectors
for this kind of workload.</p>
<p>The base latency, the time it takes to emit the window results, was in
the ballpark of 500 milliseconds, but latency would often take hikes due
to occasional Major GC's (which are not unreasonably long with the G1),
up to 10 seconds in the borderline cases (where the pipeline barely
keeps up), and still recover back to a second or two. We also noticed
the effects of JIT compilation in the borderline cases: the pipeline
would start out with a constantly increasing latency, but then after
around two minutes, its performance would improve and the latency would
make a full recovery.</p>
<p>Go to <a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part 2: the Batch Pipeline Benchmarks</a>.</p>
<p><em>If you enjoyed reading this post, check out Jet at
<a href="https://github.com/hazelcast/hazelcast-jet">GitHub</a> and give us a
star!</em></p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/05/25/grcp">Processing 10M queries / second on a single node using Jet and gRPC</a></h1><p class="post-meta">May 25, 2020</p><div class="authorBlock"><p class="post-authorName"><a target="_blank" rel="noreferrer noopener">Frantisek Hartman, Marko Topolnik</a></p></div></header><article class="post-content"><div><span><p>Implementing data processing pipelines occasionally requires calling an
external service, for example: predicting/classifying based on a ML
model, looking up records from a database or a full-text search engine,
and using a dedicated platform that computes financial risk exposure.</p>
<p>These are the typical reasons why you'd want to have some processing
done outside of the Hazelcast Jet infrastructure:</p>
<ul>
<li>The service already exists</li>
<li>The service is implemented by another team or in a different language</li>
<li>The deployment of the service needs to be independent of the
deployment of the pipeline (e.g. you need to update ML model without
modifying the pipeline)</li>
<li>Scaling the service independently of the processing pipeline</li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="grpc"></a><a href="#grpc" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>gRPC</h2>
<p><a href="https://grpc.io/">gRPC</a> is an RPC system that trades a bit of
convenience for much better performance, and also includes first-class
support for several critical concerns. Quoting from their website:</p>
<blockquote>
<p>gRPC is a modern open source high performance RPC framework that can run
in any environment. It can efficiently connect services in and across
data centers with pluggable support for load balancing, tracing, health
checking and authentication.</p>
</blockquote>
<p>With gRPC you define the service endpoint and the messages using
Protocol Buffers (Protobuf) and its interface description language
(IDL). The tooling then generates code for the server, serialization and
deserialization of the messages, and the client. Hazelcast Jet's gRPC
module makes it convenient to use the generated code to call the
endpoint from a Jet pipeline.</p>
<p>The gRPC framework provides several RPC types, but most commonly used
are unary RPC and bidirectional streaming RPC. Unary RPC is what's
usually called just &quot;RPC&quot;: the client sends a request and receives a
response. With bidirectional streaming RPC, the client starts a single
request, writes any number of messages to the request stream. Then the
client receives any number of messages in the response stream, which
makes it more similar to a messaging system with 2 topics than RPC.</p>
<h2><a class="anchor" aria-hidden="true" id="using-grpc-from-a-jet-pipeline"></a><a href="#using-grpc-from-a-jet-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Using gRPC from a Jet Pipeline</h2>
<p>With Hazelcast Jet 4.1 we have released first-class support for
accessing gRPC endpoints from Jet pipelines. In this post we investigate
the performance of the <a href="https://github.com/grpc/grpc-java">gRPC-Java</a>
framework and the effects of various settings on maximum throughput both
in vanilla gRPC and in a Jet environment.</p>
<p>Let’s start with an example of calling a gRPC endpoint from Jet.  For
example, given this protobuf definition:</p>
<pre><code class="hljs css language-proto"><span class="hljs-class"><span class="hljs-keyword">service</span> <span class="hljs-title">Greeter</span> </span>{
  <span class="hljs-comment">// Sends a greeting rpc SayHello</span>
  (HelloRequest) returns (HelloReply) {}
}
</code></pre>
<p>We can create the following service factory:</p>
<pre><code class="hljs css language-java"><span class="token keyword">var</span> greeterService <span class="token operator">=</span> <span class="token function">unaryService</span><span class="token punctuation">(</span> <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span>
  <span class="token class-name">ManagedChannelBuilder</span><span class="token punctuation">.</span><span class="token function">forAddress</span><span class="token punctuation">(</span><span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">usePlaintext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
  channel <span class="token operator">-></span> <span class="token class-name">GreeterGrpc</span><span class="token punctuation">.</span><span class="token function">newStub</span><span class="token punctuation">(</span>channel<span class="token punctuation">)</span><span class="token operator">::</span><span class="token function">sayHello</span>
<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>The first lambda returns a client channel builder. Jet uses the builder
you provide to create gRPC channels, which correspond to network
connections. You can set any desired configuration options on the
builder, such as compression and encryption. These will turn out to be
relevant to our investigation.</p>
<p>The second lambda takes a gRPC client channel obtained from the builder
and returns a function which Jet will call for each item of the
pipeline. The class <code>GreeterGrpc</code> is auto-generated by gRPC and its
<code>newStub</code> method creates the client stub. You can change the default
settings before returning it from the lambda. We won’t modify this
further in our investigation.</p>
<p>Once you have constructed a <code>ServiceFactory</code>, you can pass it to a Jet
pipeline stage transform such as <code>mapUsingServiceAsync</code>. Jet uses the
factory to create instances of the service, and passes these to the
lambda you provide. Here's an example:</p>
<pre><code class="hljs css language-java"><span class="token class-name">BatchStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Integer</span><span class="token punctuation">></span></span> stage <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
stage<span class="token punctuation">.</span><span class="token function">mapUsingServiceAsync</span><span class="token punctuation">(</span>greeterServiceFactory<span class="token punctuation">,</span> <span class="token punctuation">(</span>service<span class="token punctuation">,</span> item<span class="token punctuation">)</span> <span class="token operator">-></span>
    service<span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token class-name">HelloRequest</span><span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setValue</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<p>The word <code>Async</code> in the stage transform name means that the lambda
function must return a <code>CompletableFuture&lt;T&gt;</code>, and in our case
<code>service.call()</code> returns just that.</p>
<h2><a class="anchor" aria-hidden="true" id="the-environment"></a><a href="#the-environment" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The environment</h2>
<p>We ran the benchmarks in AWS on
2 instances of type c5.9xlarge. This instance type has:</p>
<ul>
<li>36 vCPUs</li>
<li>76 GiB of RAM</li>
<li>10 Gigabit network.</li>
</ul>
<p>One instance ran a single Hazelcast Jet
member, the other one ran the gRPC server.</p>
<h2><a class="anchor" aria-hidden="true" id="the-grpc-benchmarks"></a><a href="#the-grpc-benchmarks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The gRPC Benchmarks</h2>
<p>The gRPC-Java framework provides its own set of
<a href="https://github.com/grpc/grpc-java/tree/master/benchmarks">benchmarks</a>.
We will use these to establish a baseline to compare with the
performance of our Jet pipeline. The benchmark consists of running a
gRPC server and a client.</p>
<p>We issued this command to start the server:</p>
<pre><code class="hljs css language-bash">./grpc-benchmarks/bin/qps_server --address=<span class="hljs-variable">$SERVER</span> --transport=netty_epoll
</code></pre>
<p>And to run the client:</p>
<pre><code class="hljs css language-bash">grpc-benchmarks/bin/qps_client --address=<span class="hljs-variable">$SERVER</span> --transport=netty_epoll \
  --channels=36 --outstanding_rpcs=64 --client_payload=8 --server_payload=8
</code></pre>
<p>The parameters relevant to our investigation are:</p>
<ul>
<li>--transport - possible values are netty_epoll and netty_nio,
netty_epoll should be most performant, but it is only available on
Linux</li>
<li>--channels - number of channels (i.e., network connections) to create
on the client side</li>
<li>--outstanding_rpcs - how many requests per channel to submit without
waiting for a response</li>
<li>--client_payload and --server_payload - size of the payload sent in a
request/response</li>
<li>--streaming_rpcs - when present, uses a bidirectional streaming
endpoint, otherwise a unary endpoint</li>
</ul>
<p>Here's an example of of the output:</p>
<pre><code class="hljs css language-text">Channels:                          36
Outstanding RPCs per Channel:      64
Server Payload Size:                8
Client Payload Size:                8
50%ile Latency (in micros):      1951
90%ile Latency (in micros):      3695
95%ile Latency (in micros):      5183
99%ile Latency (in micros):     10559
99.9%ile Latency (in micros):   15807
Maximum Latency (in micros):    37375
QPS:                           986128
</code></pre>
<p>Our focus for this benchmark was throughput, so we focused on the QPS
metric - Queries Per Second. The results are quite interesting: we could
clearly identify the number of channels as the most important factor.
Using 72-108 channels increased the throughput by 20x-30x, compared to
the baseline of one channel. Since we're using very small messages,
per-message overhead inside the gRPC layer dominates over the network
limits. This is why you can achieve more throughput by adding more
channels.</p>
<p><img src="/blog/assets/2020-05-25-grpc-channels.png" alt="Througput for channels, at 128 outstanding RPCs"></p>
<p>The number of outstanding RPCs seems to have a sweet spot with a value
of 128 or 256 for this particular configuration of instance type,
network and benchmark parameters. With very low message processing time,
the optimal number of outstanding RPCs is mostly dictated by the number
of in-flight messages in the network layer. To a first approximation,
this number equals network throughput in terms of messages per second,
multiplied by the roundtrip latency of one message. Our messages are
very small, so the optimal number of outstanding RPCs is quite high.</p>
<p>As for the transport epoll and NIO transport types - it seems that for
unary calls they both reach similar maximum performance, but overall,
NIO is better.  For streaming calls, epoll wins in maximum performance
with similar results using 108 channels and 128 outstanding RPCs or 72
channels and 256 outstanding RPCs. With other settings there doesn’t
seem to be a clear winner.</p>
<p><img src="/blog/assets/2020-05-25-grpc-epoll_nio.png" alt="netty_epoll vs netty_nio"></p>
<p>All charts present best results for various configurations of number of
channels, outstanding RPC etc. The full data is available in this
<a href="https://docs.google.com/spreadsheets/d/1psjHF5ZRlxYAwxn4LA_XhvYKB0KuLXMHW8iEjrUAteE/edit#gid=63601685">spreadsheet</a>
.</p>
<h2><a class="anchor" aria-hidden="true" id="jet-benchmarks"></a><a href="#jet-benchmarks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Jet Benchmarks</h2>
<p>We created 2 types of workloads on the gRPC server side:</p>
<ul>
<li>Very fast computation (integer multiplication)</li>
<li>CPU-bound task taking 10 ms</li>
</ul>
<p>We ran each workload as:</p>
<ul>
<li>Unary RPC</li>
<li>Bidirectional streaming RPC</li>
</ul>
<p>In the Jet pipeline we don’t have the exact same parameters as in the
gRPC benchmark, but there are similar ones:</p>
<ul>
<li>Local parallelism of the <code>mapUsingServiceAsync</code> step dictates the
number of mapping processors, each processor has its own channel
instance so this is roughly equivalent to the number of channels.</li>
<li>The parameter <code>maxConcurrentOps</code> specifies how many concurrent
asynchronous mapping operations each Jet processor can issue without
waiting for a response. This gives you control over the same aspect as
the number of outstanding RPCs, but in a less direct way.</li>
</ul>
<p>This is the pipeline for a unary service, all the other benchmark
pipelines follow the same pattern:</p>
<pre><code class="hljs css language-java"><span class="token class-name">Pipeline</span> p <span class="token operator">=</span> <span class="token class-name">Pipeline</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">BatchStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Integer</span><span class="token punctuation">></span></span> stage <span class="token operator">=</span> p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">intSource</span><span class="token punctuation">(</span>jobBatchSize<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
stage<span class="token punctuation">.</span><span class="token function">mapUsingServiceAsync</span><span class="token punctuation">(</span>unaryService<span class="token punctuation">,</span>
       maxConcurrentOps<span class="token punctuation">,</span> <span class="token boolean">true</span><span class="token punctuation">,</span>
       <span class="token punctuation">(</span>service<span class="token punctuation">,</span> item<span class="token punctuation">)</span> <span class="token operator">-></span> service<span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span><span class="token class-name">HelloRequest</span><span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setValue</span><span class="token punctuation">(</span>item<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">setLocalParallelism</span><span class="token punctuation">(</span>localParallelism<span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token class-name">AggregateOperations</span><span class="token punctuation">.</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">observable</span><span class="token punctuation">(</span>runId<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="fast-computation-benchmark"></a><a href="#fast-computation-benchmark" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fast computation benchmark</h2>
<p>This benchmark is very similar to the benchmark from gRPC repo. It is
doing minimal work on the gRPC server side and measures overhead of the
Jet pipeline and network.</p>
<p>First let’s compare this with the gRPC benchmark. We achieve similar
(slightly lower) results for unary RPC, and about half for bidirectional
streaming. It’s good that we don’t get e.g. 10x less, but the drop for
bidirectional streaming is rather surprising.</p>
<p>Best throughput results achieved in gRPC benchmark and Jet benchmark:</p>
<p><img src="/blog/assets/2020-05-18-grpc-vs-jet.png" alt="gRPC benchmark and Jet pipeline"></p>
<p>Similar to the gRPC benchmark, we can observe that to achieve maximum
throughput we need to increase the number of channels and the
maxConcurrentOps parameter.</p>
<p><img src="/blog/assets/2020-05-25-fast_unary_bidi.png" alt="unary vs bidirectional"></p>
<h2><a class="anchor" aria-hidden="true" id="batch-endpoint"></a><a href="#batch-endpoint" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Batch endpoint</h2>
<p>We also tried each RPC with a modified message type containing a batch
of messages in combination with <code>mapUsingServiceAsynchBatched</code>. This is
not always possible to do - you need to be able to change the interface
of the server, but if you can, it provides a huge boost to the
throughput.</p>
<p>Jet uses what has been variously called natural or <a href="https://mechanical-sympathy.blogspot.com/2011/10/smart-batching.html">smart
batching</a>.
Typically, a batching algorithm creates batches of fixed size or waits
for a fixed time to create a batch. This increases latency in
low-throughput scenarios. Smart batching instead creates a batch from
whatever items came in while the previous batch was being processed.
When the traffic is low, this results in small batches (including single
items), preserving the best possible latency, and as the traffic grows,
larger batches automatically form, up to the limit set by the
maxBatchSize parameter.</p>
<p>After modifying the endpoint to work with batches, our results improved
dramatically:</p>
<ol>
<li>Smart batching increased the throughput by roughly 7x.</li>
<li>We needed less channels (i.e., less system resources) to reach the
maximum throughput.</li>
<li>Even though bidirectional streaming remained a winner, its advantage
over unary became very slim.</li>
</ol>
<p>Note that for this benchmark there is no maxConcurrentOps setting,
because <code>mapUsingServiceAsyncBatched</code> doesn't have it as a configurable
option.</p>
<p><img src="/blog/assets/2020-05-25-fast_unary_bidirectional_batching.png" alt="unary vs bidirectional with batching"></p>
<h2><a class="anchor" aria-hidden="true" id="cpu-bound-task-of-10-ms"></a><a href="#cpu-bound-task-of-10-ms" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>CPU bound task of 10 ms</h2>
<p>This benchmark simulates a task that takes 10 ms to complete and is
CPU-bound.  Because our testing machine has 36 vCPUs, the theoretical
maximum throughput is 3600 requests/s.</p>
<p>Interestingly, we can see that unary RPC reaches the maximum possible
throughput for all settings, however bidirectional only for 36 channels.
The results don’t change significantly for different maxConcurrentOps
settings.</p>
<p><img src="/blog/assets/2020-05-25-10ms_unary_bidirectional.png" alt="10ms task unary vs bidirectional"></p>
<p>In this scenario, smart batching no longer has a significant edge. Unary
only gets close to a maximum with 72 channels. Bidirectional achieves
maximum throughput with both 36 and 72 channels.</p>
<p><img src="/blog/assets/2020-05-25-10ms_unary_bidirectional_batching.png" alt="10ms task unary vs bidirectional"></p>
<h2><a class="anchor" aria-hidden="true" id="conclusion"></a><a href="#conclusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>We can draw several conclusions from our investigation:</p>
<ul>
<li>There is no silver bullet, the results vary significantly with
workload type, network speed and latency, resources available (e.g. on
a local development machine the benchmark yields different results).</li>
<li>Bidirectional streaming is faster than unary, roughly 2x in our case.</li>
<li>For both unary and bidirectional streaming endpoints increasing the
number of channels provides better throughput, number of CPUs on the
gRPC server side might be a good starting point.</li>
<li>Batched endpoints make a big difference, allowing much higher
throughput with less channels.</li>
<li>The execution duration of the task also makes a significant
difference, especially when taking longer than the network roundtrip
(our 10 ms CPU bound task).</li>
</ul>
<p>So in any case you should test in your environment and your own workload
to find the most advantageous setup.</p>
<h2><a class="anchor" aria-hidden="true" id="links"></a><a href="#links" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Links</h2>
<p>For a full guide on how to use Jet's gRPC functionality and API details,
please see the <a href="/docs/how-tos/grpc">manual</a>.</p>
<p>The benchmark code is published on
<a href="https://github.com/frant-hartm/hazelcast-jet-grpc-benchmark">github.com</a></p>
<p>The spreadsheet with all the results can be seen
<a href="https://docs.google.com/spreadsheets/d/1psjHF5ZRlxYAwxn4LA_XhvYKB0KuLXMHW8iEjrUAteE/edit#gid">here</a>.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/05/18/spark-jet">How Hazelcast Jet Compares to Apache Spark</a></h1><p class="post-meta">May 18, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/voloda" target="_blank" rel="noreferrer noopener">Vladimir Schreiner</a></p><div class="authorPhoto"><a href="https://twitter.com/voloda" target="_blank" rel="noreferrer noopener"><img src="https://3l0wd94f0qdd10om8642z9se-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/speaker-vladimir-schreiner-e1551380845855-170x170.jpg" alt="Vladimir Schreiner"/></a></div></div></header><article class="post-content"><div><span><p>“How Jet compares to Spark” and “why should I choose Jet over Spark” are
arguably the most frequent questions I’ve been asked during the talks
and workshops. While it is hard to assess the product fit without
focusing on a concrete use-case, I’d still like to compare concepts and
architecture used under the hood of both frameworks.</p>
<p>Versions considered: <a href="https://jet-start.sh/download">Hazelcast Jet 4.1</a>
and <a href="https://spark.apache.org/downloads.html">Apache Spark 2.4.5</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="computations-modeled-as-graphs"></a><a href="#computations-modeled-as-graphs" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Computations Modeled as Graphs</h2>
<p>Apache Spark and Hazelcast Jet (referred to as “frameworks”) are both
tools for clustered computing. They are applicable mostly for analytical
(OLAP) applications, including those that apply a series of processing
steps to many uniform data records (such as lines in a file, rows in a
table or records appended to a stream), as one example.</p>
<p>Both frameworks build on the principles of dataflow programming: a user
builds an application by chaining high-level coarse-grained operators
such as map, join or aggregate. The operators form a network that can be
modeled as a graph (<a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic
graphs</a> or DAG to
be specific) where nodes represent steps in the computation and edges
represent data exchange.</p>
<p>The dataflow model has some important properties that both frameworks
use for scaling and fault-tolerance:</p>
<ul>
<li>Pipeline Parallelism: operators can work independently, in parallel.</li>
<li>Data Parallelism: a single operator can run in multiple instances,
each instance processing a particular data partition</li>
<li>No Shared State: each operator instance manages its state exclusively.
There is no shared state to coordinate access to or to replicate.
Moreover, the state is only determined by the input data. As a result,
the operator can be recovered by replaying the input data.</li>
</ul>
<p>Spark and Jet differ in how they use and execute the DAG as explained in
the next section but fundamentally: no matter which API you use (RDDs,
Spark SQL or a Pipeline API of Jet), <strong>the physical execution plan is a
DAG representing the dataflow</strong>.</p>
<h2><a class="anchor" aria-hidden="true" id="staged-x-continuous-execution-mode"></a><a href="#staged-x-continuous-execution-mode" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Staged x Continuous Execution Mode</h2>
<p>In Spark, the DAG nodes represent execution stages. A stage must be
fully completed before Spark starts the next one. In Jet, DAG represents
connected operators. Jet executes all DAG nodes concurrently.</p>
<p>Let’s use a textbook OLAP example to elaborate: the log analysis (a
real-world application of notorious word count). Data from the access
logs are aggregated over different grouping keys, such as counting the
web sessions over several web applications using shared session id.</p>
<p>This is the Spark and Jet code to load the data, pre-process (parse) and
aggregate it:</p>
<p>Spark RDD API (Java)</p>
<pre><code class="hljs css language-java">sc<span class="token punctuation">.</span><span class="token function">textFile</span><span class="token punctuation">(</span><span class="token string">"/path/to/input/"</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span><span class="token class-name">LineIterator</span><span class="token operator">::</span><span class="token keyword">new</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">mapToPair</span><span class="token punctuation">(</span>s <span class="token operator">-></span> <span class="token keyword">new</span> <span class="token class-name">Tuple2</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token punctuation">></span></span><span class="token punctuation">(</span>s<span class="token punctuation">,</span> <span class="token number">1L</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">reduceByKey</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token class-name">Function2</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">,</span> <span class="token class-name">Long</span><span class="token punctuation">,</span> <span class="token class-name">Long</span><span class="token punctuation">></span></span><span class="token punctuation">)</span> <span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span> <span class="token operator">-></span> a <span class="token operator">+</span> b<span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">saveAsTextFile</span><span class="token punctuation">(</span><span class="token string">"/path/to/output/"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>Jet Pipeline API (Java)</p>
<pre><code class="hljs css language-java">p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">Sources</span><span class="token punctuation">.</span><span class="token function">files</span><span class="token punctuation">(</span><span class="token string">"/path/to/input/"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span><span class="token class-name">LogLine</span><span class="token operator">::</span><span class="token function">parse</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span><span class="token function">wholeItem</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">files</span><span class="token punctuation">(</span><span class="token string">"/path/to/output/"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="spark-and-staged-execution"></a><a href="#spark-and-staged-execution" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Spark and Staged Execution</h3>
<p>Spark splits the computation to non-overlapping stages. A reading stage
and a group-and-aggregate stage, in our case. During the reading stage,
Spark workers fetch data from disk files, parse it and cache it in the
cluster memory. Spark schedules more tasks if the source can be read in
parallel (e.g. data is partitioned). All reading stage tasks must be
finished before the first aggregating task is started.</p>
<p>This is the DAG representing execution stages (
<a href="https://www.tutorialkart.com/apache-spark/dag-and-physical-execution-plan/">source</a>).</p>
<p><img src="/blog/assets/2020-05-18-spark-dag-stages.svg" alt="Spark Staged Execution"></p>
<p>Staged execution was designed to support an iterative analytics use-case
where the results of one stage stay cached in a cluster memory to be
reused by a following step in the analysis. This makes Spark a popular
choice for ML research where a data scientist gradually evolves the
dataset with new experiments, evicting the data when their Spark session
is over. It is also a powerful debugging tool.</p>
<p>On the other hand, staged execution doesn’t perform well for
latency-sensitive use-cases, namely stream processing.</p>
<p>Streaming data is continuously incrementing. Staged execution is however
designed for finite datasets. Whole input must be read before Spark
starts subsequent steps. Spark Streaming works around this by batching
the input data, e.g. creating finite chunks from an infinite stream.
Buffering adds to the job latency as the data are waiting for the batch
to fill, staying idle.</p>
<p>The stages are planned and scheduled again and again for every batch.
The overhead of the planning process increases the latency further.</p>
<p>Another latency penalty comes if the data partitions are not balanced
evenly. If a single partition of data takes longer to read or process,
it would block the whole job from progressing since the next stage can’t
be started. Jet would be impacted by this scenario, too, but it can
still provide early results – in-complete, indicative results based on
already processed partitions.</p>
<h3><a class="anchor" aria-hidden="true" id="jet-and-continuous-execution"></a><a href="#jet-and-continuous-execution" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Jet and Continuous Execution</h3>
<p>Jet executes all DAG nodes concurrently. The DAG is deployed to all
cluster nodes when the job is submitted and runs until a termination.
The instances of running DAG nodes, called Processors, then run in
parallel and continuously exchange data. For partitioned data sets, the
data partitions are evenly distributed among available processors (see
the
<a href="https://jet-start.sh/docs/architecture/distributed-computing">docs</a>).</p>
<p>This is the DAG representing the execution plan for the log aggregation.
Jet would create multiple instances of each and route data among it
following the routing strategy (<a href="https://jet-start.sh/docs/next/architecture/distributed-computing">source</a>):</p>
<p><img src="/blog/assets/2020-05-18-jet-dag.svg" alt="Jet Execution DAG"></p>
<p>A reading Processor keeps fetching data from the data source and sends
it to a downstream channel immediately. The channel routes data to the
respective aggregating processor, following the grouping key. The
aggregating processor is observing the input channel and updates the
aggregate with each input item. It’s an application concern to specify
when the aggregator emits the aggregate downstream – with every input
item, after a period of time, after the whole dataset has been processed
or based on a data-driven trigger.</p>
<p>The continuous execution model is a natural fit for streaming use-cases
that stress low latency. Jet Jobs can keep millisecond latencies on a
large scale.</p>
<p>Spark has introduced the continuous execution mode in 2.4. The mode is
still experimental and is limited to stateless operators (mapping,
filtering) so it wasn’t considered for this comparison.</p>
<h2><a class="anchor" aria-hidden="true" id="in-memory-execution"></a><a href="#in-memory-execution" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>In-Memory Execution</h2>
<p>Spark and Jet both rely on an in-memory execution. That means that <em>data
transfer</em> and <em>execution state</em> both use the cluster RAM.</p>
<h3><a class="anchor" aria-hidden="true" id="data-transfer"></a><a href="#data-transfer" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Data Transfer</h3>
<p>In-memory data transfer means that the data between the consecutive DAG
nodes are exchanged using shared memory instead of a disk (shuffling the
data among cluster nodes still requires a network, of course).</p>
<p>Spark exchanges data between stages by saving the complete output of an
upstream stage in the memory of the worker to be used as an input of a
downstream stage. Jet uses in-memory queues to connect upstream and
downstream Processors.</p>
<p>That makes Spark more memory demanding as it caches the whole dataset
exchanged between two steps which can easily be hundreds of GB of data.
Spark workers are therefore able to spill data to disk not to run out of
memory. Jet processors run all in parallel and exchange data
continuously. The in-flight data are no more than a few thousand
records.</p>
<h3><a class="anchor" aria-hidden="true" id="execution-state"></a><a href="#execution-state" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Execution State</h3>
<p>Execution state refers to the temporary data of the computation, such as
the value of an ongoing aggregation or join. Both Jet and Spark keep the
state data on heap by default.</p>
<p>Spark can however also place execution state off-heap and even spill it
to disk. It can, therefore, perform calculations that require a large
state such as joins or sorts on huge datasets. For Jet, the execution
state must fit to cluster memory.</p>
<h2><a class="anchor" aria-hidden="true" id="dedicated-x-shared-resources"></a><a href="#dedicated-x-shared-resources" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Dedicated x Shared Resources</h2>
<p>Spark applications running in a cluster are isolated from each other.
Jet shares the cluster resources between applications (called Jobs). No
approach is “the right one”. It’s trading-off isolation and performance.</p>
<h3><a class="anchor" aria-hidden="true" id="spark-assigns-dedicated-resources"></a><a href="#spark-assigns-dedicated-resources" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Spark Assigns Dedicated Resources</h3>
<p>For each application, Spark runs dedicated processes for both scheduling
and execution.</p>
<p>The processes are created with the resources (CPU, memory and disk)
allocated to the application upon startup and reserved during job
lifetime. After the Spark Application ends, the processes are terminated
and the resources are freed.</p>
<p>This design clearly favours isolation. A noisy application doesn’t
affect the neighbours using the same computer. It can, however, lead to
overprovisioning as an Application holds allocated resources even if it
doesn’t require it.</p>
<p>Spark was designed in the age of Hadoop – huge clusters of heterogeneous
machines running many workloads. Multi-tenancy was, therefore, a
first-level design concern.</p>
<h3><a class="anchor" aria-hidden="true" id="jet-shares-resources"></a><a href="#jet-shares-resources" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Jet Shares Resources</h3>
<p>The Jet cluster is also formed by multiple member processes. Those
processes are started when the cluster starts and aren’t coupled with a
lifecycle of individual hosted Job.</p>
<p>Jobs share the cluster resources and run in a <a href="https://jet-start.sh/docs/architecture/execution-engine">cooperative
mode</a>. Each job
does a small amount of work and yields to the next one. Job is removed
from this round-robin after it finishes.</p>
<p>This design leads to efficient resource utilization. All jobs get a fair
amount of CPU time. If a job gets idle (e.g. waiting for more input
data), it simply backs off and Jet excludes it from the round-robin
rotation for a few milliseconds, giving busy Jobs more CPU to keep up.</p>
<p>Resource sharing is of course prone to noisy neighbours – a greedy job
can starve others. To prevent this, Jet recommends starting a cluster
per tenant or even per job, increasing the isolation to a Spark level.</p>
<h3><a class="anchor" aria-hidden="true" id="shared-datasets"></a><a href="#shared-datasets" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Shared Datasets</h3>
<p>Another benefit of job sharing processes is exchanging data over the
shared memory.</p>
<p>A job can load and pre-process data, caching it in cluster memory (Jet
comes with distributed storage). The cached collection then becomes a
source for further processing jobs, leading to <a href="https://hazelcast.com/resources/jet-0-4-vs-spark-flink-batch-benchmark/">significant performance
gains</a>
from reading the local memory instead of a remote data source. Another
use-case is shared reference data (such as lookup tables or parameters)
or queues connecting the output stream of one job to an input of another
one.</p>
<p>Spark applications run in isolated processes so they must use external
storage to exchange data.</p>
<h2><a class="anchor" aria-hidden="true" id="conclusion"></a><a href="#conclusion" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conclusion</h2>
<p>Jet and Spark are frameworks that use principles of dataflow programming
to run analytical computations on clusters of machines for scalability
and resiliency.</p>
<p>They differ in how they implement and execute the data flow. Jet’s
design favours streaming use-cases that benefit from the low-latency
continuous execution. Spark can spill data to disk and isolates jobs on
a process level. Therefore it’s a good fit for large, multi-tenant
clusters.</p>
<p>Other areas worth comparing are the cluster architecture and APIs. They
will be covered in the next part of the article.</p>
</span></div></article></div><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/04/29/jet-41-is-released">Jet 4.1 is Released</a></h1><p class="post-meta">April 29, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="http://twitter.com/cgencer" target="_blank" rel="noreferrer noopener">Can Gencer</a></p><div class="authorPhoto"><a href="http://twitter.com/cgencer" target="_blank" rel="noreferrer noopener"><img src="https://pbs.twimg.com/profile_images/1187734846749196288/elqWdrPj_400x400.jpg" alt="Can Gencer"/></a></div></div></header><article class="post-content"><div><span><p>We are happy to present the new release of Hazelcast Jet 4.1. Here's a
quick overview of the new features.</p>
<h2><a class="anchor" aria-hidden="true" id="extended-grpc-support"></a><a href="#extended-grpc-support" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Extended gRPC Support</h2>
<p>We've applied the lessons learned from the Jet-Python integration and
made it easier to integrate a Jet pipeline with <a href="https://grpc.io">gRPC</a>
services. The utility class <code>GrpcServices</code> introduces two new
<code>ServiceFactory</code>s you can use with the <code>mapUsingServiceAsync</code> transform.
Using this feature can be a significant performance boost vs. using the
sync <code>mapUsingService</code> call.</p>
<p>Here's a quick example on how you can use the gRPC service factory:</p>
<pre><code class="hljs css language-java"><span class="token keyword">var</span> greeterService <span class="token operator">=</span> <span class="token function">unaryService</span><span class="token punctuation">(</span>
    <span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token class-name">ManagedChannelBuilder</span><span class="token punctuation">.</span><span class="token function">forAddress</span><span class="token punctuation">(</span><span class="token string">"localhost"</span><span class="token punctuation">,</span> <span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">usePlaintext</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    channel <span class="token operator">-></span> <span class="token class-name">GreeterGrpc</span><span class="token punctuation">.</span><span class="token function">newStub</span><span class="token punctuation">(</span>channel<span class="token punctuation">)</span><span class="token operator">::</span><span class="token function">sayHello</span>
<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token class-name">Pipeline</span> p <span class="token operator">=</span> <span class="token class-name">Pipeline</span><span class="token punctuation">.</span><span class="token function">create</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token class-name">TestSources</span><span class="token punctuation">.</span><span class="token function">items</span><span class="token punctuation">(</span><span class="token string">"one"</span><span class="token punctuation">,</span> <span class="token string">"two"</span><span class="token punctuation">,</span> <span class="token string">"three"</span><span class="token punctuation">,</span> <span class="token string">"four"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">mapUsingServiceAsync</span><span class="token punctuation">(</span>greeterService<span class="token punctuation">,</span> <span class="token punctuation">(</span>service<span class="token punctuation">,</span> input<span class="token punctuation">)</span> <span class="token operator">-></span> <span class="token punctuation">{</span>
    <span class="token class-name">HelloRequest</span> request <span class="token operator">=</span> <span class="token class-name">HelloRequest</span><span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">setName</span><span class="token punctuation">(</span>input<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">return</span> service<span class="token punctuation">.</span><span class="token function">call</span><span class="token punctuation">(</span>request<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>In addition to the unary gRPC service, we support bidirectional
streaming as well as request batching. For a more in-depth look, see the
<a href="/docs/how-tos/grpc">Call a gRPC Service how-to guide</a> and the <a href="/docs/design-docs/007-grpc-support">design
document</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="transactional-jdbc-and-jms-sinks"></a><a href="#transactional-jdbc-and-jms-sinks" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Transactional JDBC and JMS sinks</h2>
<p>In Jet 4.0 we added support for <a href="/blog/2020/02/20/transactional-processors">transactional sources and
sinks</a> through the use of
two-phase commit. We're now extending this support for two additional
sinks: JDBC and JMS. The support requires the broker or the database to
support XA transactions. To test your database's support for XA
transactions, we've also released a <a href="/docs/how-tos/xa">how-to guide</a>.</p>
<p>You can also see a full summary of sinks and sources and the variety of
transaction support on the <a href="/docs/api/sources-sinks#summary">sources and sinks
page</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="code-deployment-improvements"></a><a href="#code-deployment-improvements" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Code Deployment Improvements</h2>
<p>When you're deploying a Jet job programmatically (not using the <code>jet submit</code> command-line tool), you must add every class the job needs to
the job's configuration. So far, Jet has supported adding classes one by
one with <code>JobConfig.addClass()</code> and that wouldn't add any of the class's
nested classes. This was especially problematic for anonymous classes,
which you can't even refer to from Java code. In 4.1 we improved
<code>addClass()</code> so that it adds all the nested classes and we added
<code>JobConfig.addPackage()</code> so you can add the whole package in a
one-liner, and don't have to manually maintain the list of classes as
you develop your pipeline. Take a look at the <a href="/docs/design-docs/001-code-deployment-improvements">design
document</a> for more
details.</p>
<h2><a class="anchor" aria-hidden="true" id="job-scoped-serializer-support"></a><a href="#job-scoped-serializer-support" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Job-Scoped Serializer Support</h2>
<p>So far Jet has had a pain point in terms of serialization. The objects
that travel through the pipeline must sometimes be sent from one cluster
member to the other, so they must be serialized. You can let the object
implement <code>Serializable</code>, but that's inefficient. If you wanted to use
a better serialization scheme, you had to register a serializer object
with the Jet cluster and restart the whole cluster.</p>
<p>It is now possible to attach a serializer directly to the job you're
submitting.</p>
<pre><code class="hljs css language-java"><span class="token class-name">JobConfig</span> config <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">JobConfig</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
  <span class="token punctuation">.</span><span class="token function">registerSerializer</span><span class="token punctuation">(</span><span class="token class-name">Person</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> <span class="token class-name">PersonSerializer</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

jet<span class="token punctuation">.</span><span class="token function">newJob</span><span class="token punctuation">(</span>pipeline<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>Jet will use these serializers only inside the job. You can read more
about how serialization in Hazelcast Jet works in the <a href="/docs/api/serialization">serialization
guide</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="protocol-buffers-support"></a><a href="#protocol-buffers-support" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Protocol Buffers Support</h2>
<p>Having added the job-level serializers, we also added an extra layer of
convenience to use <a href="https://developers.google.com/protocol-buffers">Google Protocol
Buffers</a> for
serialization. You just need to write a simple class that delegates the
work to the Protobuf compiler-generated serializer class (<code>Person</code> in
this case):</p>
<pre><code class="hljs css language-java"><span class="token keyword">class</span> <span class="token class-name">PersonSerializer</span> <span class="token keyword">extends</span> <span class="token class-name">ProtobufSerializer</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Person</span><span class="token punctuation">></span></span> <span class="token punctuation">{</span>

    <span class="token keyword">private</span> <span class="token keyword">static</span> <span class="token keyword">final</span> <span class="token keyword">int</span> TYPE_ID <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>

    <span class="token class-name">PersonSerializer</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
        <span class="token keyword">super</span><span class="token punctuation">(</span><span class="token class-name">Person</span><span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">,</span> TYPE_ID<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">}</span>
</code></pre>
<p>For more information, see the <a href="/docs/api/serialization#google-protocol-buffers">serialization guide</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="spring-boot-starter"></a><a href="#spring-boot-starter" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Spring Boot Starter</h2>
<p>Spring Boot is a framework that helps you create standalone Spring-based
applications that just run. Spring Boot provides auto-configuration of
some of the commonly used libraries through spring-boot-starters.
Hazelcast Jet now provides its own <a href="https://github.com/hazelcast/hazelcast-jet-contrib/tree/master/hazelcast-jet-spring-boot-starter">Spring Boot
Starter</a>
which can be used to auto-configure and start a Hazelcast Jet instance.</p>
<p>Just by adding the starter dependency to your Spring Boot application,
you can start a <code>JetInstance</code> with the default configuration. If you
want to customize the configuration, just add a configuration file
(<code>hazelcast-jet.yaml</code>) to your classpath or working directory. The
starter will pick it up and configure your Hazelcast Jet instance.  If
you want a client instance, add the client configuration file
(<code>hazelcast-client.yaml</code>).</p>
<p>For more details, see the <a href="/docs/design-docs/004-spring-boot-starter">design
document</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="kubernetes-operator-and-openshift-support"></a><a href="#kubernetes-operator-and-openshift-support" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Kubernetes Operator and OpenShift Support</h2>
<p>With version 4.1 we are introducing our <a href="https://operatorhub.io/?keyword=jet">Hazelcast Jet Kubernetes
Operator</a>. It's available for both
Hazelcast Jet open-source and Enterprise editions. Hazelcast Jet
Enterprise Operator is also a certified by Red Hat and available on the
Red Hat Marketplace.</p>
<h2><a class="anchor" aria-hidden="true" id="discovery-support-for-microsoft-azure"></a><a href="#discovery-support-for-microsoft-azure" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Discovery Support for Microsoft Azure</h2>
<p>We have extended the list of cloud environments where Hazelcast Jet
instances are able to automatically discover each other and form a
cluster. Self-discovery now works in the Microsoft Azure environment.
Here's a quick example on how to enable it:</p>
<pre><code class="hljs css language-yaml"><span class="hljs-attr">hazelcast:</span>
  <span class="hljs-attr">network:</span>
    <span class="hljs-attr">join:</span>
      <span class="hljs-attr">multicast:</span>
        <span class="hljs-attr">enabled:</span> <span class="hljs-literal">false</span>
      <span class="hljs-attr">azure:</span>
        <span class="hljs-attr">enabled:</span> <span class="hljs-literal">true</span>
        <span class="hljs-attr">tag:</span> <span class="hljs-string">TAG-NAME=HZLCAST001</span>
        <span class="hljs-attr">hz-port:</span> <span class="hljs-number">5701</span><span class="hljs-number">-5703</span>
</code></pre>
<p>For more details, please see the <a href="/docs/operations/discovery#azure-cloud">discovery
guide</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="full-release-notes"></a><a href="#full-release-notes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Full Release Notes</h2>
<p>Members of the open source community that appear in these release notes:</p>
<ul>
<li>@TomaszGaweda</li>
<li>@caioguedes</li>
<li>@SapnaDerajeRadhakrishna</li>
</ul>
<p>Thank you for your valuable contributions!</p>
<h3><a class="anchor" aria-hidden="true" id="new-features"></a><a href="#new-features" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>New Features</h3>
<ul>
<li>[jms] Exactly-once guarantee for JMS sink (#1813)</li>
<li>[jdbc] Exactly-once guarantee for JDBC sink (#1813)</li>
<li>[core] JobConfig.addClass() automatically adds nested classes to the
job (#1932)</li>
<li>[core] JobConfig.addPackage() adds a whole Java package to the job
(#1932, #2077)</li>
<li>[core] Job-scoped serializer deployment (#2020, #2038, #2039, #2043,
#2071, #2075, #2082, #2190)</li>
<li>[core] [006] Protobuf serializer support (#2100)</li>
<li>[pipeline-api] [007] Support gRPC for mapUsingService (#2095, #2185)</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="enhancements"></a><a href="#enhancements" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Enhancements</h3>
<ul>
<li>[jet-cli] Use log4j2 instead of log4j (#1981)</li>
<li>[jet-cli] Simplify default log output (#2047)</li>
<li>[core] Add useful error message when serializer not registered (#2061)</li>
<li>[jet-cli] Add hazelcast-azure cluster self-discovery plugin to the
fat JAR in the distribution archive (#2079)</li>
<li>[pipeline-api] First-class support for inner hash join (@TomaszGaweda
#2089)</li>
<li>[core] When Jet starts up, it now logs the cluster name (@caioguedes
#2105)</li>
<li>[core] Add useful error message when trying to deploy a JDK class with
JobConfig (#2108)</li>
<li>[core] Implement JobConfig.toString (@SapnaDerajeRadhakrishna #2152)</li>
<li>[core] Do not destroy Observable on shutdown (#2170)</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="fixes"></a><a href="#fixes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Fixes</h3>
<ul>
<li>[core] Don't send the interrupt signal to blocking threads when a job
is terminating (#1971)</li>
<li>[core] Consistently prefer YAML over XML config files when both
present (#2033)</li>
</ul>
<h3><a class="anchor" aria-hidden="true" id="breaking-changes"></a><a href="#breaking-changes" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Breaking Changes</h3>
<ul>
<li>[avro] Replace Supplier<Schema> with just Schema for Avro Sink (#2005)</li>
<li>[jms] Reorder parameters in JMS source so the lambda comes last
(#2062)</li>
<li>[jet-cli] Change smart routing (connecting to all cluster members)
default to disabled (#2104)</li>
<li>[pipeline-api] For xUsingServiceAsync transforms, reduce the default
number of concurrent service calls per processor. Before: 256; now: 4.
(#2204)</li>
</ul>
</span></div></article></div><div class="docs-prevnext"><a class="docs-next" href="/blog/page2/">Next →</a></div></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><div style="text-align:left"><a href="/" class="nav-home"><img src="/img/logo-light.svg" alt="Hazelcast Jet" width="200" height="40"/></a><div style="margin-left:12px"><a class="github-button" href="https://github.com/hazelcast/hazelcast-jet" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star On GitHub</a></div></div><div><h5>Docs</h5><a href="/docs/get-started/intro">Get Started</a><a href="/docs/concepts/dag">Concepts</a><a href="/docs/tutorials/kafka">Tutorials</a><a href="/docs/architecture/distributed-computing">Architecture</a><a href="/docs/operations/installation">Operations Guide</a><a href="/docs/enterprise">Enterprise Edition</a></div><div><h5>Community</h5><a href="https://groups.google.com/forum/#!forum/hazelcast-jet" target="_blank" rel="noreferrer noopener">Google Groups</a><a href="http://stackoverflow.com/questions/tagged/hazelcast-jet" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://slack.hazelcast.com">Slack</a></div><div><h5>Latest From the Blog</h5><a href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a><a href="/blog/2020/08/05/gc-tuning-for-jet">Sub-10 ms Latency in Java: Concurrent GC with Green Threads</a><a href="/blog/2020/07/16/designing-evergreen-cache-cdc">Designing an Evergreen Cache with Change Data Capture</a><a href="/blog/2020/07/14/jet-42-is-released">Jet 4.2 is Released</a><a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch</a></div><div><h5>More</h5><a href="https://github.com/hazelcast/hazelcast-jet">GitHub Project</a><a href="http://hazelcast.com/company/careers/">Work at Hazelcast</a><a href="/license">License</a></div></section><section class="copyright">Copyright © 2020 Hazelcast Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '79d1e4941621b9fd761d279d4d19ed69',
                indexName: 'hazelcast-jet',
                inputSelector: '#search_input_react',
                algoliaOptions: {"facetFilters":["language:en","version:4.2"]}
              });
            </script></body></html>