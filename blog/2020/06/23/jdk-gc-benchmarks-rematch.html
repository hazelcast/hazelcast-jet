<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch · Hazelcast Jet</title><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="generator" content="Docusaurus"/><meta name="description" content="This post is a part of a series:"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch · Hazelcast Jet"/><meta property="og:type" content="website"/><meta property="og:url" content="https://jet-start.sh/blog/2020/06/23/jdk-gc-benchmarks-rematch"/><meta property="og:description" content="This post is a part of a series:"/><meta property="og:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://jet-start.sh/blog/atom.xml" title="Hazelcast Jet Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://jet-start.sh/blog/feed.xml" title="Hazelcast Jet Blog RSS Feed"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-158279495-1', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,500,600"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600,700,800"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-block-buttons.js"></script><script type="text/javascript" src="https://plausible.io/js/plausible.js" async="" defer="" data-domain="jet-start.sh"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/prism.css"/><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo-dark.svg" alt="Hazelcast Jet"/></a><a href="/versions"><h3>4.5</h3></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/get-started/intro" target="_self">Docs</a></li><li class=""><a href="/download" target="_self">Download</a></li><li class=""><a href="/demos" target="_self">Demos</a></li><li class=""><a href="https://github.com/hazelcast/hazelcast-jet" target="_self">GitHub</a></li><li class=""><a href="https://slack.hazelcast.com/" target="_self">Community</a></li><li class="siteNavGroupActive"><a href="/blog/" target="_self">Blog</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>All posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">All posts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/blog/2021/03/17/billion-events-per-second">Billion Events Per Second with Millisecond Latency: Streaming Analytics at Giga-Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/06/enabling-full-text-search">Enabling Full-text Search with Change Data Capture in a Legacy Application</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/08/05/gc-tuning-for-jet">Sub-10 ms Latency in Java: Concurrent GC with Green Threads</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/16/designing-evergreen-cache-cdc">Designing an Evergreen Cache with Change Data Capture</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/14/jet-42-is-released">Jet 4.2 is Released</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Performance of Modern Java on Data-Heavy Workloads: Batch Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/25/grcp">Processing 10M queries / second on a single node using Jet and gRPC</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/18/spark-jet">How Hazelcast Jet Compares to Apache Spark</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/29/jet-41-is-released">Jet 4.1 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/01/upgrading-to-jet-40">Upgrading to Jet 4.0</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/30/ml-inference">Machine Learning Inference at Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/02/jet-40-is-released">Jet 4.0 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/02/20/transactional-processors">Transactional connectors in Hazelcast Jet</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/01/28/new-website">Announcing New Documentation Website</a></li><li class="navListItem"><a class="navItem" href="/blog/2019/11/12/stream-deduplication">Stream Deduplication with Hazelcast Jet</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="lonePost"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch</a></h1><p class="post-meta">June 23, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener">Marko Topolnik</a></p><div class="authorPhoto"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener"><img src="https://i.imgur.com/xuavzce.jpg" alt="Marko Topolnik"/></a></div></div></header><div><span><p>This post is a part of a series:</p>
<ul>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Part 1 (Intro and high-throughput streaming
benchmark)</a></li>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Part 2 (batch workload benchmark)</a></li>
<li>Part 3 (you are here)</li>
<li><a href="/blog/2020/08/05/gc-tuning-for-jet">Part 4 (concurrent GC with green threads)</a></li>
<li><a href="/blog/2021/03/17/billion-events-per-second">Part 5 (billion events per second)</a></li>
</ul>
<p>This is a followup on Part 1 of the blog post series we started earlier
this month, analyzing the performance of modern JVMs on workloads that
are relevant to the use case of real-time stream processing.</p>
<p>As a quick recap, in Part 1 we tested the basic functionality of
<a href="https://github.com/hazelcast/hazelcast-jet">Hazelcast Jet</a> (sliding
window aggregation) on two types of workload: lightweight with a focus
on low latency, and heavyweight with a focus on the data pipeline
keeping up with high throughput and large aggregation state. For the
low-latency benchmarks we chose the JDK 14 as the most recent stable
version and three of its garbage collectors: Shenandoah, ZGC, and G1 GC.</p>
<p>Our finding that Shenandoah apparently fared worse than the other GCs
attracted some reactions, most notably from the Shenandoah team who
reproduced our finding, created an
<a href="https://bugs.openjdk.java.net/browse/JDK-8247358">issue</a>, came up with
a fix, and committed it to the jdk/jdk16 repository, all in the span of
a few days. The change pertains to the heuristics that decide how much
work the GC should do in the background in order to exactly match the
applications allocation rate. This component is called the <em>pacer</em>. It
was constantly detecting it's falling behind the application, triggering
a brief &quot;panic mode&quot; in order to catch up. The fix fine-tunes the
pacer's heuristics to make the background GC work more proactive.</p>
<p>Given this quick development, we wanted to test out the effects of the
fix, but also take the opportunity to zoom in on the low-latency
streaming case and make a more detailed analysis.</p>
<p>Here are our main conclusions:</p>
<ol>
<li>ZGC is still the winner and the only GC whose 99.99th percentile
latency stayed below 10 ms across almost all of our tested range</li>
<li>Shenandoah's pacer improvement showed a very strong effect, reducing
the latency by a factor of three, but still staying well above 10 ms
except in the very lowest part of our tested range</li>
<li>G1 kept its 99.99th percentile latency below 13 ms across a wide
range of throughputs</li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="the-jdk-we-tested"></a><a href="#the-jdk-we-tested" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The JDK We Tested</h2>
<p>Since this is all so fresh, we couldn't use an existing JDK release, not
even EA, to see the effects of the fix. JDK version 14.0.2 is slated to
be released on July 14. To nevertheless make progress, we took the
source code from the jdk14u tree, at the changeset number
<a href="http://hg.openjdk.java.net/jdk-updates/jdk14u/rev/e9d41bbaea38">57869:e9d41bbaea38</a>,
and applied the changeset number
<a href="https://hg.openjdk.java.net/jdk/jdk/rev/29b4bb22b5e2">59746:29b4bb22b5e2</a>
from the main jdk tree on top of it. The jdk14u tree is where JDK 14.0.2
will be released from and the changeset 59746:29b4bb22b5e2 applies the
patch resolving the mentioned Shenandoah issue.</p>
<h2><a class="anchor" aria-hidden="true" id="the-jvm-options"></a><a href="#the-jvm-options" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The JVM Options</h2>
<p>There are two HotSpot JVM options whose default values change
automatically when you use the ZGC so we had to decide which choice to
make when testing the other garbage collectors.</p>
<ul>
<li><p><code>-XX:-UseBiasedLocking</code>: biased locking has for a while been under
criticism that it causes higher latency spikes due to bias revocation
that must be done within a GC safepoint. In the upcoming JDK version
15, biased locking will be <a href="https://openjdk.java.net/jeps/374">disabled by default and
deprecated</a>. Any low-latency Java
application should have this disabled and we disabled it in all our
measurements.</p></li>
<li><p><code>-XX:+UseNUMA</code>: Shenandoah and ZGC can query the NUMA layout of the
host machine and optimize their memory layout accordingly. The only
reason why Shenandoah doesn't do it by default is a general precaution
against suddenly changing the behavior for upgrading users, but the
precaution is no longer necessary. It will be <a href="https://openjdk.java.net/jeps/163">enabled by
default</a> in upcoming JDK versions,
and we saw no harm in enabling it in all cases as well. <strong>Late
update</strong>: G1 can also optimize for the NUMA layout, but we didn't use
<code>UseNUMA</code> for it. However, we also checked the c5.4xlarge instance
with <code>numactl</code> and it indicated that the entire machine was a single
NUMA node anyway.</p></li>
</ul>
<p>There is also a JVM feature that is simply incompatible with ZGC's
colored pointers: compressed object pointers. In other words, ZGC
applies <code>-XX:-UseCompressedOops</code> without the option to enable it.
A compressed pointer is just 32 bits long but handles heaps of up to
32 GB and it's usually beneficial to both memory usage and performance.
We left this option enabled for Shenandoah.</p>
<p>For the G1 collector, we also set <code>-XX:MaxGCPauseMillis=5</code>, same as in
the previous testing round, because the default of 200 milliseconds is
optimized for throughput and the G1 can give you much better latency
than that.</p>
<p>We performed all our tests on an EC2 c5.4xlarge instance. It has 16
vCPUs and 32 GB of RAM.</p>
<h2><a class="anchor" aria-hidden="true" id="the-data-pipeline"></a><a href="#the-data-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Data Pipeline</h2>
<p>To get a more nuanced insight into the performance, we made some
improvements to the testing code. Whereas in the first iteration we just
reported the maximum latency, this time around we wanted to capture the
entire latency profile. To this end we had to increase the number of
reports per second the pipeline outputs. Initially we set it to 10 times
per second, a number which results in too few data points for the
latency chart. The pipeline in this round emits 100 reports per second.
The event rate and the length of the time window are the same: 1 million
events per second and 10 seconds, respectively. This results in 1,000
hashtables each holding 10,000 keys as the aggregation state. We tested
across a wide range of keyset sizes, starting from 5,000 up to 105,000.</p>
<p>Note that the size of the keyset, somewhat counterintuitively, does not
affect the size of the aggregation state. As long as the 10,000 input
events received during one time slice of 10 milliseconds all use
distinct keys, the state is fixed as described above. Only in the lowest
setting, 5,000, the state is half as large since every hashtable
contains just 5,000 keys.</p>
<p>What the keyset size does affect is allocation rate. The pipeline emits
the full keyset every 10 milliseconds. For example, with 50,000 keys
that's 5,000,000 result items per second. If we add to that the rate of
the input stream (a fixed million events per second), we get a value
that is a good proxy for the overall allocation rate. This is why we
chose combined input+output rate as the x-axis value in the charts that
we'll be showing below.</p>
<p>Here is the basic code of the pipeline, available on
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/round-2/src/main/java/org/example/StreamingRound2.java">GitHub</a>:</p>
<pre><code class="hljs css language-java"><span class="token class-name">StreamStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">></span></span> source <span class="token operator">=</span> p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">longSource</span><span class="token punctuation">(</span>EVENTS_PER_SECOND<span class="token punctuation">)</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">withNativeTimestamps</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
                            <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token class-name">StreamStage</span><span class="token generics"><span class="token punctuation">&lt;</span><span class="token class-name">Tuple2</span><span class="token punctuation">&lt;</span><span class="token class-name">Long</span><span class="token punctuation">,</span> <span class="token class-name">Long</span><span class="token punctuation">></span><span class="token punctuation">></span></span> latencies <span class="token operator">=</span> source
        <span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> NUM_KEYS<span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">window</span><span class="token punctuation">(</span><span class="token function">sliding</span><span class="token punctuation">(</span>WIN_SIZE_MILLIS<span class="token punctuation">,</span> SLIDING_STEP_MILLIS<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">counting</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>kwr <span class="token operator">-></span> kwr<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> DIAGNOSTIC_KEYSET_DOWNSAMPLING_FACTOR <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span><span class="token function">mapStateful</span><span class="token punctuation">(</span><span class="token class-name">DetermineLatency</span><span class="token operator">::</span><span class="token keyword">new</span><span class="token punctuation">,</span> <span class="token class-name">DetermineLatency</span><span class="token operator">::</span><span class="token function">map</span><span class="token punctuation">)</span><span class="token punctuation">;</span>

latencies<span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>t2 <span class="token operator">-></span> t2<span class="token punctuation">.</span><span class="token function">f0</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> TOTAL_TIME_MILLIS<span class="token punctuation">)</span>
         <span class="token punctuation">.</span><span class="token function">map</span><span class="token punctuation">(</span>t2 <span class="token operator">-></span> <span class="token class-name">String</span><span class="token punctuation">.</span><span class="token function">format</span><span class="token punctuation">(</span><span class="token string">"%d,%d"</span><span class="token punctuation">,</span> t2<span class="token punctuation">.</span><span class="token function">f0</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> t2<span class="token punctuation">.</span><span class="token function">f1</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
         <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">files</span><span class="token punctuation">(</span><span class="token string">"/home/ec2-user/laten"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
latencies
      <span class="token punctuation">.</span><span class="token function">mapStateful</span><span class="token punctuation">(</span><span class="token class-name">RecordLatencyHistogram</span><span class="token operator">::</span><span class="token keyword">new</span><span class="token punctuation">,</span> <span class="token class-name">RecordLatencyHistogram</span><span class="token operator">::</span><span class="token function">map</span><span class="token punctuation">)</span>
      <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">files</span><span class="token punctuation">(</span><span class="token string">"/home/ec2-user/bench"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre>
<p>The main part, sliding window aggregation, remains the same, but the
following stages that process the results are new. We write the data to
two files: <code>laten</code>, containing all the raw latency data points, and
<code>bench</code>, containing an <a href="https://hdrhistogram.github.io/HdrHistogram/plotFiles.html">HDR
Histogram</a>
of the latencies.</p>
<p>Another key difference is that, in the original post, we measured the
latency of <em>completing</em> to emit a result set, but here we measure the
latency of <em>starting</em> to emit it. Since we are changing the size of the
output, if we kept measuring the completion latency, we'd be introducing
a different amount of application-induced latency at each data point.</p>
<p>There's another, relatively minor technical point worth mentioning:
since we tested on a cloud server instance, we used Jet's client-server
mode, which means we separately start a Jet node and then deploy the
pipeline to it using Jet's command <code>jet submit</code>. The code available on
GitHub is the client code and the Jet server code was a build from the
Jet master branch before Jet 4.2 was released. We expect all the results
to be reproducible with the <a href="https://github.com/hazelcast/hazelcast-jet/releases/download/v4.2/hazelcast-jet-4.2.tar.gz">Jet 4.2
release</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="what-exactly-we-measured"></a><a href="#what-exactly-we-measured" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>What Exactly We Measured</h2>
<p>We measured the latency as the timestamp at which the pipeline emits a
given result minus the timestamp to which the result pertains, giving us
end-to-end latency (the only kind the user actually cares about).</p>
<p>Keep especially in mind that latency does not equal a GC pause.
Normally, neither Shenandoah nor ZGC enter anything more than a
millisecond of GC pause, but their background work shares the limited
system capacity with the application. With G1 the equivalence is much
stronger and its 10-20 millisecond latencies are primarily the result of
GC pauses that long.</p>
<h2><a class="anchor" aria-hidden="true" id="the-measurements"></a><a href="#the-measurements" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>The Measurements</h2>
<p>To come up with the charts below, for each data point we let the
pipeline warm up for 20 seconds and then gathered the latencies for 4
minutes, collecting 24,000 samples.</p>
<p>Here is the latency histogram taken at 2 million items per second,
close to the bottom of our range:</p>
<p><img src="/blog/assets/2020-06-23-histo-2m.png" alt="Latency on JDK 14.0.2 pre-release, 2M items per second"></p>
<p>Unpatched Shenandoah seems like the winner, except for the single
worst-case latency. With the patch applied, latency increases sooner but
more gently and doesn't have a strong peak. ZGC comes somewhere between,
but overall all three cases show pretty similar behavior. G1 is clearly
worse and its latency exceeds the 10 ms mark before even reaching the
99th percentile. Since our pipeline emits a new result set ever 10 ms,
we shall consider 10 ms as the cutoff point: everything above 10 ms
should be considered a failure for our use case.</p>
<p>Next, let's take a look at the latencies after increasing the throughput
a bit, to 3 million items per second:</p>
<p><img src="/blog/assets/2020-06-23-histo-3m.png" alt="Latency on JDK 14.0.2 pre-release, 3M items per second"></p>
<p>Wow, what an unexpected difference! Now we can clearly see the pacer
improvement doing its thing, lowering the latency about threefold.
However, even with the improvement, Shenandoah unfortunately crosses the
10 ms mark pretty early, below the 99th percentile, and is worse than G1
at almost every percentile. ZGC and G1 score basically the same as
before.</p>
<p>Note also the very regular shape of the pale blue curve (unpatched
Shenandoah): this is a symptom of the way a single bad event trickles
down into the lower latency percentiles. For example, if one result is
late by 50 ms, that means it has already caused the next four results to
have at least the latencies of 40, 30, 20, and 10 ms, even if they would
be emitted instantaneously.</p>
<p>Next, let's zoom out to an overview of the entire range of throughputs
we benchmarked, taking the 99.99%ile as the reference point and showing
its dependence on throughput. To paint an intuitive picture, 99.99%
latency tells you that, in any span of 100 seconds you look at, you're
likely to find a latency spike at least that large. Here's the chart:</p>
<p><img src="/blog/assets/2020-06-23-latencies-jdk14.png" alt="Latencies on JDK 14.0.2 pre-release"></p>
<p>Here are some things to note:</p>
<ol>
<li>ZGC stays below 10 ms over a large part of the range, up to 8 M items
per second. This makes it not just the winner, but the only choice
for the range from 2 million to 8 million items per second.</li>
<li>The G1 collector is unphased by the differences in throughput. While
its latency is never under 10 milliseconds, it keeps its level over
the entire tested range and more. Its latency even improves a bit
with higher loads.</li>
<li>At 9.5 M items per second, ZGC shows a remarkable recovery.
Sandwiched between the latencies of 92 and 209 milliseconds, at this
exact throughput it achieves 10 ms latency! We of course thought it
was a measurement error and repeated it for three times, but the
result was consistent. Maybe there's a lesson in there for the ZGC
engineers.</li>
</ol>
<h2><a class="anchor" aria-hidden="true" id="a-sneak-peek-into-upcoming-versions"></a><a href="#a-sneak-peek-into-upcoming-versions" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>A Sneak Peek into Upcoming Versions</h2>
<p>As a preview into what's coming up in OpenJDK, we also took a look at
the <a href="https://download.java.net/java/early_access/jdk15/28/GPL/openjdk-15-ea+28_linux-x64_bin.tar.gz">Early Access release 27 of JDK
15</a>.
Shenandoah's pacer improvement is not applied in it, so to properly test
Shenandoah's prospects we used a build available at
<a href="https://builds.shipilev.net/openjdk-jdk/">builds.shipilev.net/openjdk-jdk</a>,
specifically one that reports its version as <code>build 16-testing+0-builds.shipilev.net-openjdk-jdk-b1282-20200611</code>. Out of
interest we also doubled our throughput range to capture more of the
behavior after the latency exceeds 10 milliseconds. Here's what we got:</p>
<p><img src="/blog/assets/2020-06-23-latencies-latest.png" alt="Latencies on upcoming JDK versions"></p>
<p>We can see a nice incremental improvement for the ZGC: less than 5 ms
latencies at throughputs below 5 M/s. Shenandoah's curve is even a bit
worse at 2.5-3 M per second, but generally pretty similar. At higher
loads we can see ZGC's failure mode is quite a bit more severe than
Shenandoah's, although just how bad the latency gets doesn't affect the
bottom line of a scenario where everything above 10 ms is already a
failure.</p>
<p>The wider chart also gives better insight into the stability of G1,
keeping itself below 20 ms all the way up to 20 M items per second.</p>
<p><em>If you enjoyed reading this post, check out Jet at
<a href="https://github.com/hazelcast/hazelcast-jet">GitHub</a> and give us a
star!</em></p>
</span></div></div><div class="blogSocialSection"></div></div><div class="blog-recent"><a class="button" href="/blog/">Recent posts</a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#the-jdk-we-tested">The JDK We Tested</a></li><li><a href="#the-jvm-options">The JVM Options</a></li><li><a href="#the-data-pipeline">The Data Pipeline</a></li><li><a href="#what-exactly-we-measured">What Exactly We Measured</a></li><li><a href="#the-measurements">The Measurements</a></li><li><a href="#a-sneak-peek-into-upcoming-versions">A Sneak Peek into Upcoming Versions</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div style="text-align:left"><a href="/" class="nav-home"><img src="/img/logo-light.svg" alt="Hazelcast Jet" width="200" height="40"/></a><div style="margin-left:12px"><a class="github-button" href="https://github.com/hazelcast/hazelcast-jet" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star On GitHub</a></div></div><div><h5>Docs</h5><a href="/docs/get-started/intro">Get Started</a><a href="/docs/concepts/dag">Concepts</a><a href="/docs/tutorials/kafka">Tutorials</a><a href="/docs/architecture/distributed-computing">Architecture</a><a href="/docs/operations/installation">Operations Guide</a><a href="/docs/enterprise">Enterprise Edition</a></div><div><h5>Community</h5><a href="https://groups.google.com/forum/#!forum/hazelcast-jet" target="_blank" rel="noreferrer noopener">Google Groups</a><a href="http://stackoverflow.com/questions/tagged/hazelcast-jet" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://slack.hazelcast.com">Slack</a></div><div><h5>Latest From the Blog</h5><a href="/blog/2021/03/17/billion-events-per-second">Billion Events Per Second with Millisecond Latency: Streaming Analytics at Giga-Scale</a><a href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a><a href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a><a href="/blog/2020/10/06/enabling-full-text-search">Enabling Full-text Search with Change Data Capture in a Legacy Application</a><a href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a></div><div><h5>More</h5><a href="https://github.com/hazelcast/hazelcast-jet">GitHub Project</a><a href="http://hazelcast.com/company/careers/">Work at Hazelcast</a><a href="/license">License</a></div></section><section class="copyright">Copyright © 2021 Hazelcast Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '79d1e4941621b9fd761d279d4d19ed69',
                indexName: 'hazelcast-jet',
                inputSelector: '#search_input_react',
                algoliaOptions: {"facetFilters":["language:en","version:4.5"]}
              });
            </script></body></html>