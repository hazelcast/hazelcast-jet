<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Performance of Modern Java on Data-Heavy Workloads: Batch Processing · Hazelcast Jet</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="This post is a part of a series:"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Performance of Modern Java on Data-Heavy Workloads: Batch Processing · Hazelcast Jet"/><meta property="og:type" content="website"/><meta property="og:url" content="https://jet-start.sh/blog/2020/06/09/jdk-gc-benchmarks-part2"/><meta property="og:description" content="This post is a part of a series:"/><meta property="og:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://jet-start.sh/img/Hazelcast-Jet-Logo-Blue_Dark.jpg"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://jet-start.sh/blog/atom.xml" title="Hazelcast Jet Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://jet-start.sh/blog/feed.xml" title="Hazelcast Jet Blog RSS Feed"/><script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
              (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
              m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

              ga('create', 'UA-158279495-1', 'auto');
              ga('send', 'pageview');
            </script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:300,400,500,600"/><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,500,600,700,800"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><script type="text/javascript" src="/js/code-block-buttons.js"></script><script type="text/javascript" src="https://plausible.io/js/plausible.js" async="" defer="" data-domain="jet-start.sh"></script><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/prism.css"/><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/logo-dark.svg" alt="Hazelcast Jet"/></a><a href="/versions"><h3>4.4</h3></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/docs/get-started/intro" target="_self">Docs</a></li><li class=""><a href="/download" target="_self">Download</a></li><li class=""><a href="/demos" target="_self">Demos</a></li><li class=""><a href="https://github.com/hazelcast/hazelcast-jet" target="_self">GitHub</a></li><li class=""><a href="https://slack.hazelcast.com/" target="_self">Community</a></li><li class="siteNavGroupActive"><a href="/blog/" target="_self">Blog</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>All posts</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">All posts</h3><ul class=""><li class="navListItem"><a class="navItem" href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/10/06/enabling-full-text-search">Enabling Full-text Search with Change Data Capture in a Legacy Application</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/08/05/gc-tuning-for-jet">Sub-10 ms Latency in Java: Concurrent GC with Green Threads</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/16/designing-evergreen-cache-cdc">Designing an Evergreen Cache with Change Data Capture</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/07/14/jet-42-is-released">Jet 4.2 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Performance of Modern Java on Data-Heavy Workloads: The Low-Latency Rematch</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Performance of Modern Java on Data-Heavy Workloads: Batch Processing</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Performance of Modern Java on Data-Heavy Workloads: Real-Time Streaming</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/25/grcp">Processing 10M queries / second on a single node using Jet and gRPC</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/05/18/spark-jet">How Hazelcast Jet Compares to Apache Spark</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/29/jet-41-is-released">Jet 4.1 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/04/01/upgrading-to-jet-40">Upgrading to Jet 4.0</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/30/ml-inference">Machine Learning Inference at Scale</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/03/02/jet-40-is-released">Jet 4.0 is Released</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/02/20/transactional-processors">Transactional connectors in Hazelcast Jet</a></li><li class="navListItem"><a class="navItem" href="/blog/2020/01/28/new-website">Announcing New Documentation Website</a></li><li class="navListItem"><a class="navItem" href="/blog/2019/11/12/stream-deduplication">Stream Deduplication with Hazelcast Jet</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer postContainer blogContainer"><div class="wrapper"><div class="lonePost"><div class="post"><header class="postHeader"><h1 class="postHeaderTitle"><a href="/blog/2020/06/09/jdk-gc-benchmarks-part2">Performance of Modern Java on Data-Heavy Workloads: Batch Processing</a></h1><p class="post-meta">June 9, 2020</p><div class="authorBlock"><p class="post-authorName"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener">Marko Topolnik</a></p><div class="authorPhoto"><a href="https://twitter.com/mtopolnik" target="_blank" rel="noreferrer noopener"><img src="https://i.imgur.com/xuavzce.jpg" alt="Marko Topolnik"/></a></div></div></header><div><span><p>This post is a part of a series:</p>
<ul>
<li><a href="/blog/2020/06/09/jdk-gc-benchmarks-part1">Part 1 (Intro and high-throughput streaming
benchmark)</a></li>
<li>Part 2 (you are here)</li>
<li><a href="/blog/2020/06/23/jdk-gc-benchmarks-rematch">Part 3 (low-latency benchmark)</a></li>
<li><a href="/blog/2020/08/05/gc-tuning-for-jet">Part 4 (concurrent GC with green threads)</a></li>
</ul>
<h2><a class="anchor" aria-hidden="true" id="batch-pipeline-benchmark"></a><a href="#batch-pipeline-benchmark" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Batch Pipeline Benchmark</h2>
<p>A batch pipeline processes a finite amount of stored data. There are no
running results, we need the output of the aggregate function applied to
the entire dataset. This changes our performance requirements: the key
factor in streaming, latency, doesn't exist here since we are not
processing data in real time. The only metric that matters is the total
run time of the pipeline.</p>
<p>For this reason we considered the Parallel GC as a relevant candidate.
In the first testing round, on a single node, it actually delivered the
best throughput (but only after GC tuning). However, it achieves that
throughput at the expense of GC pause duration. In a cluster, whenever
any node enters a GC pause, it stalls the whole data pipeline. Since
individual nodes enter GC pauses at different times, the amount of time
spent in GC goes up with every node you add to the cluster. We explored
this effect by comparing single-node tests with tests in a three-node
cluster.</p>
<p>On the flip side, we did not consider the experimental low-latency
collectors in this round since their very short GC pauses have no effect
on the test result, and they achieve them at the expense of throughput.</p>
<h3><a class="anchor" aria-hidden="true" id="single-node-benchmark-the-pipeline"></a><a href="#single-node-benchmark-the-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Single-Node Benchmark: The Pipeline</h3>
<p>For the single-node batch benchmark we used this simple pipeline, full
code on
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/master/src/main/java/org/example/BatchBenchmark.java">GitHub</a>:</p>
<pre><code class="hljs css language-java">p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span>longSource<span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment">// Introduced in Jet 4.2</span>
 <span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> NUM_KEYS<span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token function">summingLong</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>e <span class="token operator">-></span> <span class="token punctuation">(</span>e<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token number">0</span>xFF_FFFFL<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre>
<p>The source is again a self-contained mock source that just emits a
sequence of <code>long</code> numbers and the key function is defined so that the
grouping key cycles through the key space: 0, 1, 2, ..., <code>NUM_KEYS</code>, 0,
1, 2, ... This means that, over the first cycle, the pipeline observes
all the keys and builds up a fixed data structure to hold the
aggregation results. Over the following cycles it just updates the
existing data. This aligns perfectly with the Generational Garbage
Hypothesis: the objects either last through the entire computation or
are short-lived temporary objects that become garbage very soon after
creation.</p>
<p>We let our source emit 400 million items and had 100 million distinct
keys, so we cycled four times over the same keys.</p>
<p>The <code>.rebalance()</code> operator changes Jet's default <a href="/docs/concepts/dag#group-and-aggregate-transform-needs-data-partitioning">two-stage
aggregation</a>
to single-stage. It exhibited more predictable behavior in our
benchmark.</p>
<p>We also tested a variant where the aggregate operation uses a boxed
<code>Long</code> instance as state, producing garbage every time the running score
is updated. In this case many objects die after having spent substantial
time in the old generation. For this variant we had to reduce the number
of keys to 70 million, with 100 million the GC pressure was too high.</p>
<p>For the batch pipeline we didn't focus on the low-latency collectors
since they have nothing to offer in this case. Also, because we saw
earlier that JDK 14 performs much the same as JDK 11, we just ran one
test to confirm it, but otherwise focused on JDK 8 vs. JDK 11 and
compared the JDK 8 default Parallel collector with G1.</p>
<h3><a class="anchor" aria-hidden="true" id="single-node-benchmark-the-results"></a><a href="#single-node-benchmark-the-results" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Single-Node Benchmark: The Results</h3>
<p>For single-node testing, we ran the benchmark on a laptop with 16 GB RAM
and a 6-core Intel Core i7. We used a heap size of 10 GB.</p>
<p>Initially we got very bad performance out of the Parallel collector and
had to resort to GC tuning. For this purpose we highly recommend using
VisualVM and its Visual GC plugin. When you set the frame rate to the
highest setting (10 FPS), you can enjoy a very fine-grained visual
insight into how the interplay between your application's allocation and
the GC works out. By watching these live animations for a while, we
realized that the main issue was a too large slice of RAM given to the
new generation. By default the ratio between Old and New generations is
just 2:1, and it is not dynamically adaptable at runtime. Based on this
we decided to try with <code>-XX:NewRatio=8</code> and it completely changed the
picture. Now Parallel was turning in the best times overall. We also
used <code>-XX:MaxTenuringThreshold=2</code> to reduce the copying of data between
the Survivor spaces, since in the pipeline the temporary objects die
pretty soon.</p>
<p>Now, on to the results. The only relevant metric in this batch pipeline
benchmark is the time for the job to complete. To visualize the results
we took the reciprocal of that, so the charts show throughput in items
per second. Here are our results on a single node:</p>
<p><img src="/blog/assets/2020-06-01-batch-mutable.png" alt="Single-node Batch pipeline with garbage-free aggregation"></p>
<p><img src="/blog/assets/2020-06-01-batch-boxed.png" alt="Single-node Batch pipeline with garbage-producing aggregation"></p>
<p>Comparing the two charts we can see that garbage-free aggregation gave a
throughput boost of around 30-35%, and that's despite the larger keyset
we used for it. G1 on JDK 8 was the worst performer and the fine-tuned
Parallel on JDK 11 was the best. G1 on JDK 11 wasn't far behind. Note
that we didn't have to touch anything in the configuration of G1, which
is an important fact. GC tuning is highly case-specific, the results may
dramatically change with e.g., more data, and it must be applied to the
entire cluster, making it specifically tuned for one kind of workload.</p>
<p>Here's the performance of the default Parallel GC compared to the tuned
version we used for testing:</p>
<p><img src="/blog/assets/2020-06-01-batch-parallel.png" alt="Throughput of the Parallel Collector with and without tuning"></p>
<p>With 10 GB of heap it failed completely, stuck in back-to-back Full GC
operations each taking about 7 seconds. With more heap it managed to
make some progress, but was still hampered with very frequent Full GCs.
Note that we got the above results for the most favorable case, with
garbage-free aggregation.</p>
<h3><a class="anchor" aria-hidden="true" id="three-node-cluster-benchmark-the-pipeline"></a><a href="#three-node-cluster-benchmark-the-pipeline" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Three-Node Cluster Benchmark: The Pipeline</h3>
<p>To properly benchmark in the cluster, we had to use a bit more complex
<a href="https://github.com/mtopolnik/jet-gc-benchmark/blob/master/src/main/java/org/example/ClusterBatchBenchmark.java">pipeline</a>:</p>
<pre><code class="hljs css language-java">p<span class="token punctuation">.</span><span class="token function">readFrom</span><span class="token punctuation">(</span><span class="token function">longSource</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">flatMap</span><span class="token punctuation">(</span>n <span class="token operator">-></span> <span class="token punctuation">{</span>
     <span class="token class-name">Long</span><span class="token punctuation">[</span><span class="token punctuation">]</span> items <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Long</span><span class="token punctuation">[</span>SOURCE_STEP<span class="token punctuation">]</span><span class="token punctuation">;</span>
     <span class="token class-name">Arrays</span><span class="token punctuation">.</span><span class="token function">setAll</span><span class="token punctuation">(</span>items<span class="token punctuation">,</span> i <span class="token operator">-></span> n <span class="token operator">+</span> i<span class="token punctuation">)</span><span class="token punctuation">;</span>
     <span class="token keyword">return</span> <span class="token function">traverseArray</span><span class="token punctuation">(</span>items<span class="token punctuation">)</span><span class="token punctuation">;</span>
 <span class="token punctuation">}</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">rebalance</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">groupingKey</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n <span class="token operator">%</span> NUM_KEYS<span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">aggregate</span><span class="token punctuation">(</span><span class="token class-name">AggregateOperations</span><span class="token punctuation">.</span><span class="token function">summingLong</span><span class="token punctuation">(</span>n <span class="token operator">-></span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">filter</span><span class="token punctuation">(</span>e <span class="token operator">-></span> e<span class="token punctuation">.</span><span class="token function">getKey</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">%</span> <span class="token number">1_000_000</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span>
 <span class="token punctuation">.</span><span class="token function">writeTo</span><span class="token punctuation">(</span><span class="token class-name">Sinks</span><span class="token punctuation">.</span><span class="token function">logger</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">;</span>
</code></pre>
<p>Since the source is non-parallel, we applied some optimizations so it
doesn't become a bottleneck. We let the source emit the numbers 0, 10,
20, ... and then applied a parallelized <code>flatMap</code> stage that
interpolates the missing numbers. We also used <code>rebalance()</code> between the
source and <code>flatMap</code>, spreading the data across the cluster. We applied
rebalancing again before entering the main stage, keyed aggregation.
After the aggregation stage we first reduce the output to every
millionth key-value pair and then send it to the logger. We used one
billion data items and a keyset of half a billion.</p>
<p>Same as on single-node, we tested both this pipeline, with a garbage-free
aggregation, and a modified one with garbage-producing aggregation.</p>
<h3><a class="anchor" aria-hidden="true" id="three-node-cluster-benchmark-the-results"></a><a href="#three-node-cluster-benchmark-the-results" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Three-Node Cluster Benchmark: The Results</h3>
<p>We performed this benchmark on an AWS cluster of three c5d.4xlarge
instances. They have 16 virtualized CPU cores and 32 GB of RAM. The
network is 10 Gbit/s. Here are the results:</p>
<p><img src="/blog/assets/2020-06-01-batch-cluster-mutable.png" alt="3-Node Batch pipeline with garbage-free aggregation"></p>
<p><img src="/blog/assets/2020-06-01-batch-cluster-boxed.png" alt="3-Node Batch pipeline with garbage-producing aggregation"></p>
<p>In passing, let's note the overall increase in throughput compared to
single-node benchmarks, about three times. That's the advantage of
distributed processing. As for collectors, G1 on JDK 11 is the clear
winner in both tests. Another striking result is the almost nonexistent
bar for G1 on JDK 8, however there's a deeper story here that affects
other measurements as well, for example the apparent advantage of
Parallel GC on JDK 8 vs. JDK 11. It has to do with the effect we noted
at the outset: a GC pause on any one member halts the processing on the
entire cluster. G1 on JDK 8 enters very long GC pauses, more than a
minute. This is enough to trigger the cluster's failure detector and
consider the node dead. The job fails, the cluster reshapes itself, and
then the job restarts on just two nodes. This, naturally, fails even
sooner because there's more data on each member. In the meantime the
kicked-out node has rejoined, so the job restarts on two nodes again,
but different ones. We end up in an endless loop of job restarts.</p>
<p>The Parallel collector's GC pauses stopped short of bringing down the
cluster, but it fared significantly worse than in single-node tests.
Here it was 30% behind the G1 on JDK 11. With a bigger cluster this
would get even worse.</p>
<p>Compared to all other tests, it is surprising to see Parallel on JDK 8
win over JDK 11, however this is due to a very lucky coincidence that,
in those particular test runs, the Full GC pauses got synchronized on
all nodes, parallelizing the effort of the GC. Clearly, this is not a
reliable effect.</p>
<p>Even though in the particular benchmark setup which we report here, we
didn't observe the catastrophic consequence of long GC pauses on cluster
stability while using the Parallel collector, it is more of a chance
outcome. In other tests, where we used a larger heap and more data, or
the same heap but with less headroom left, the Parallel collector did
cause the same damage. However, even when it doesn't cause outright
failure, the charts show the advantage it had on a single node has
disappeared. You can expect the results to get worse with each further
node you add to the cluster.</p>
<p>The JDK 11 G1 collector, on the other hand, was producing GC pauses of a
sufficiently short duration that the rest of the pipeline didn't get
stalled. The pipeline has mechanisms that dampen out short hiccups and
as long as the GC pauses are within acceptable limits (up to some 150
ms), the effect of GC stays local.</p>
<p><em>If you enjoyed reading this post, check out Jet at
<a href="https://github.com/hazelcast/hazelcast-jet">GitHub</a> and give us a
star!</em></p>
</span></div></div><div class="blogSocialSection"></div></div><div class="blog-recent"><a class="button" href="/blog/">Recent posts</a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#batch-pipeline-benchmark">Batch Pipeline Benchmark</a><ul class="toc-headings"><li><a href="#single-node-benchmark-the-pipeline">Single-Node Benchmark: The Pipeline</a></li><li><a href="#single-node-benchmark-the-results">Single-Node Benchmark: The Results</a></li><li><a href="#three-node-cluster-benchmark-the-pipeline">Three-Node Cluster Benchmark: The Pipeline</a></li><li><a href="#three-node-cluster-benchmark-the-results">Three-Node Cluster Benchmark: The Results</a></li></ul></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div style="text-align:left"><a href="/" class="nav-home"><img src="/img/logo-light.svg" alt="Hazelcast Jet" width="200" height="40"/></a><div style="margin-left:12px"><a class="github-button" href="https://github.com/hazelcast/hazelcast-jet" data-icon="octicon-star" data-count-href="/facebook/docusaurus/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star On GitHub</a></div></div><div><h5>Docs</h5><a href="/docs/get-started/intro">Get Started</a><a href="/docs/concepts/dag">Concepts</a><a href="/docs/tutorials/kafka">Tutorials</a><a href="/docs/architecture/distributed-computing">Architecture</a><a href="/docs/operations/installation">Operations Guide</a><a href="/docs/enterprise">Enterprise Edition</a></div><div><h5>Community</h5><a href="https://groups.google.com/forum/#!forum/hazelcast-jet" target="_blank" rel="noreferrer noopener">Google Groups</a><a href="http://stackoverflow.com/questions/tagged/hazelcast-jet" target="_blank" rel="noreferrer noopener">Stack Overflow</a><a href="https://slack.hazelcast.com">Slack</a></div><div><h5>Latest From the Blog</h5><a href="/blog/2021/02/03/jet-44-is-released">Jet 4.4 Released</a><a href="/blog/2020/10/23/jet-43-is-released">Jet 4.3 Released</a><a href="/blog/2020/10/06/enabling-full-text-search">Enabling Full-text Search with Change Data Capture in a Legacy Application</a><a href="/blog/2020/09/18/cdc-meets-stream-processing">Change Data Capture meets Stream Processing</a><a href="/blog/2020/08/05/gc-tuning-for-jet">Sub-10 ms Latency in Java: Concurrent GC with Green Threads</a></div><div><h5>More</h5><a href="https://github.com/hazelcast/hazelcast-jet">GitHub Project</a><a href="http://hazelcast.com/company/careers/">Work at Hazelcast</a><a href="/license">License</a></div></section><section class="copyright">Copyright © 2021 Hazelcast Inc.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>
                document.addEventListener('keyup', function(e) {
                  if (e.target !== document.body) {
                    return;
                  }
                  // keyCode for '/' (slash)
                  if (e.keyCode === 191) {
                    const search = document.getElementById('search_input_react');
                    search && search.focus();
                  }
                });
              </script><script>
              var search = docsearch({
                
                apiKey: '79d1e4941621b9fd761d279d4d19ed69',
                indexName: 'hazelcast-jet',
                inputSelector: '#search_input_react',
                algoliaOptions: {"facetFilters":["language:en","version:4.4"]}
              });
            </script></body></html>